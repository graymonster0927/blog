---
title: UA Assessing Language Models on Unsolved Questions
date: 2026-02-10
categories: [论文, '202602']
tags: [大模型]
---

## UA Assessing Language Models on Unsolved Questions
### [**原论文**](https://arxiv.org/pdf/2508.17580) 

### 本质剖析 (The Gist)
  当前的基准测试都是一些已经解决的困难问题, 但其实希望LLM解决的是现实世界高频且开放式的问题, 所以用当前基准测试跑出来的高分大模型真的能很好解决现实问题吗? 论文提出了新的评估框架与数据集 —— UQ（Unanswered Questions）未解决问题基准，并配套构建验证器与平台，试图把模型评估从“考试式”推向“现实世界问题驱动式”。
### 核心解读 (Insights)  
  当前语言模型基准测试主要分为基于已知答案的“考试型”和基于真实查询的“应用型”，前者常导致测试与现实脱节，后者则易出现高饱和与操纵风险。这种双重局限解释了为何模型能力常被误判，强调了构建既真实又具高难度的动态评估场之必要性, 也就是下面使​​基准测试有意义的两个最重要的属性:
  1. 困难性：必须对最先进模型仍具挑战性。
  2. 现实性：问题来自真实世界，且答案具有现实价值。

  UQ 正是围绕这两点构建。

  未解问题给基准测试带来了两个主要挑战：由于缺乏真实答案，我们需要：（1）验证问题的难度和质量；（2）评估不同模型生成的候选解决方案
  
  UQ 有三部分组成
  1. 数据集（§2）：该数据集包含一系列未解决的问题，这些问题经过三阶段流程筛选：（i）使用互动信号（例如浏览量、投票数、评论数、发布时间）对未解答的问题进行基于规则的筛选；（ii）基于逻辑逻辑模型（LLM）对问题进行筛选，筛选标准包括定义清晰性、难度、易懂性和客观性；（iii）由来自 STEM 和非 STEM 领域的博士级标注员进行人工审核。最终得到一系列涵盖数学、物理、计算机科学理论、生物声学、科幻、神话等多个领域的高质量、高难度开放性问题。
  
  2. 验证器（§3）：一组基于 LLM 的验证策略，旨在评估候选 LLM 解决方案。我们利用了前沿模型在验证解决方案方面优于生成解决方案的观察结果，并且这种生成器与验证器之间的差距体现了跨数据集的迁移性。我们探索了一种候选答案的分层验证框架，该框架结合了：（i）低级检查，例如事实/逻辑正确性和问答循环一致性；（ii）中级抽样策略，包括重复和迭代判断；以及（iii）高级聚合策略，例如多数投票、一致投票和顺序验证。UQ验证者作为评估周期的第一阶段，试图排除错误答案，以便进行人工验证。

  3. 平台（§4，uq.stanford.edu）：一个实时开放的平台，用于完成模型评估周期。它托管未解决的问题以及候选模型答案，UQ验证结果和完整的溯源信息（提示/元数据）以确保可复现性。它还作为用户和……的中心枢纽。

#### UQ 数据集构建

  **第一阶段：规则筛选（Interaction-based Filtering）**

  目标：确保问题真实、有价值、且确实未被解决。
  从社区网站收集未解答问题，设定规则：
  * 年龄：问题必须 ≥ 2 年。这排除了可能很快就能回答的新问题，并允许足够的时间来吸引关注。
  * 浏览量：问题必须有 ≥ 200–2000 次浏览（网站相关）。这筛选出兴趣低的问题。
  * 投票数：问题必须有 ≥5–75 的净点赞数（网站相关），以排除参与度低的问题。
  * 排名靠前：问题必须位于每个网站未回答问题中按投票数排名前 10% 的问题。这条规则主要在高流量网站（如数学）上触发，因为这类网站有大量符合条件的问题，以进一步筛选质量。
  * 没有答案：问题必须没有答案（而不仅仅是候选答案未被原始发布者接受）。这增加了问题未解决的几率。


  **第二阶段：LLM 质量评估筛选**

  使用 LLM 对问题进行独立三次评估, 每次使用双模型方法，其中通用模型（例如，GPT-4o）首先尝试回答每个候选问题，然后推理模型（例如，o4-mini）根据以下五个标准结合生成的答案来评估问题, 标准包括：

  * 定义清晰：问题是否明确且清晰（是/否）。
  * 候选正确性导致的难度：尝试回答的正确可能性（0-100%）。
  * 按可解性难度：领域专家解决该问题的可能性（0-100%）。
  * 可及性：问题在原则上是否逻辑上合理且可解（是/否）。
  * 目标：真实答案是否客观且可验证（是/否）。

  评估后再次筛选, 我们计算数值标准的平均值（答案正确性和专家可解性），并对二元标准（定义良好、可接近、客观）进行一致投票。我们认为满足所有二元标准、答案正确性平均值为≤40%、专家可解性平均值为≤70%的问题为高质量问题，并将其提交给人进行审核


  **第三阶段：人工博士级审核**
  
  来自 STEM 与非 STEM 领域博士人工复核：审查问题质量, 查看 OpenAI o3、Gemini 2.5 Pro、Claude 3.7 Sonnet 的生成答案, 判断是否确实困难且合理, 最终得到跨数学、物理、理论 CS、生物声学、科幻、神话等领域的开放性问题。

	这样得到的数据集包含：
	钻石子集 (对上面问题再次用更高条件筛选)
	开发集（含已被接受答案）

	同时当然数据集是动态更新的, 比如社区被标记解决会同步, 如果模型提交答案被接受 那半实时的排名上会加分

#### 验证器系统（Validator Framework）

##### 一、评分器:
  评分因为没有准确答案, 不太好评, 于是论文构建了一个“无预言机验证器”。

  简单说就是选了个数据集-人类最后考试（HLE）HLE这个数据集是有答案可以评估的, 于是根据这个数据集, 他们调自己的评分器 ,当评分器在HLE标准答案评分高时  那认为是个好的评分器了

  为什么不找些人工专家去调一个好的评分器? 恩 他们说了成本太高


##### 二、验证器设计目标

  在无预言机（no oracle）的环境中：
  * **假阳性（False Positive）**：错误答案被通过
  * **假阴性（False Negative）**：正确答案被拒绝

  论文明确设计目标：
  > 优先降低假阳性（提高 Precision），而不是提高召回率。

  为什么？
  1. 未解决问题通常：本质困难, 但可能“看起来很合理”
  2. 模型容易：生成错误但貌似聪明的答案, 并给自己高分
  3. 高精度可以：减少人工专家审核成本, 避免错误答案进入排行榜


##### 三、分层验证策略设计空间
  低层次策略是指用于评估候选答案基本属性的提示技术：
  * 正确性：判断答案是否准确完整地回答问题；
  * 事实/逻辑检查：检查答案中的事实、算术和逻辑错误；
  * 循环一致性：推断出导致给定答案的问题，然后将其与原始提示进行比较。这可以检验答案是否与问题有实质性的关联。

  中层策略是通过冗余和自我审核来提高判断稳健性的方法：

  * 重复抽样：使用随机种子对验证者进行抽样，以收集多个验证结果；
  * 迭代反思：促使评判模型在多次反思迭代中重新评估并可能修改初始判断。

  高层战略是将多个判断整合为最终判决的方法：
  * 多数投票：如果大多数验证结果（例如，低级或中级策略实例）为肯定，则接受答案；
  * 一致投票：与上述类似，但只有当所有判断均为肯定时才接受答案；
  * 流水线验证：将验证策略组织成若干轮（或阶段），只有通过当前阶段的验证结果才能进入下一阶段。除非另有说明，流水线通常使用三轮验证。

  验证器多层策略组合，无论它们位于同一抽象层级还是跨越抽象层级。例如，一个简单的验证器可以先让基础模型检查其正确性，然后使用来自该模型的三个独立样本重复检查，最后通过一致投票进行聚合。
  

##### 四、一些过程中发现

> 一致投票 > 多数投票  
> 让模型“自我反思”有时候有帮助  
> 多模型一起不一定更好 低评分模型会拉低整体水平(一致投票)  

**发现 #1**：复合验证策略优于简单提示基线  
**发现 #2**：实现高精度很困难
1. 确实本来模型就解决问题能力不行
2. 不是预言机制的验证器 是个黑盒 没法调整

**发现 #3**：简单验证器表现出过度乐观和自我偏见  
**发现 #4**：复合验证器策略减轻过度乐观和自我偏差  
**发现 #5**：模型排名在验证器性能上不稳定  
总的来说 验证器只是在HLE集下调整 对真实数据集没有更好的表现 即使用了最复杂的模型o3+多次迭代+多管道 所以不能依赖这种无预言机验证器来构建模型排行榜

#### 平台
正是因为只有有限可靠的验证器不可靠, 因此他们构建了个平台, 在平台让人参与进来做最后决策 


  