---
title: 2.1 k8s概述
date: 2022-10-15
categories: [笔记, '云原生', 'k8s']
tags: [云原生]
---

# k8s概述 . [完全摘抄于这里 重要事情提前写](https://www.cnblogs.com/liconglong/p/15044124.html#_label1)

## 一、Google Borg

### （一）什么是云计算
过去对于云管理或者叫作业管理主要有两条路，一个是以Openstack为典型的虚拟化平台，一个是以谷歌 borg 为典型的基于进程的作业调度平台。
* 以Openstack为典型的虚拟化平台，是吧很多物理机装上Hypervisor，在上面启动一些虚拟机，再把这些虚拟机组成一个大的云平台，最终交付的是一个个的操作系统，在操作系统之上，可以部署应用系统。那么应用的部署、升级、管理都和云平台有比较明显的界限，所以像之前的云平台都会有比较清晰的基础架构的Infra as a service、平台的Platform as a service、SaaS Software as a service。这种云管平台虚拟机构建和业务代码部署分离，可变的基础架构使后续维护风险变大。
* 以谷歌 borg 为典型的基于进程的作业调度平台，其调用的是一个个进程，Borg本身利用了一些容器技术，比如CGroup技术、早期使用的chroot jail 技术以及后续替换成的namespace技术去做应用之间的隔离。

![](/commons/云原生/docker/image/2.1(1).png)

### （二）Google Borg
K8S是Kubernetes的简称，其是基于谷歌的Borg系统开发的。Borg是谷歌用来管理公司内部所有作业的任务管理平台。
* Borg主要支撑两类业务，一类叫做生产业务（Production Workload），例如Gmail（邮箱服务）、Google Docs（文档服务）、Web Search（搜索服务）；一类叫做离线作业（Non-prod Service），例如AI相关的、大数据相关的。生产业务要求高可用，其对资源的消耗可能并没有那么大，但是要求永远在线，而离线作业一般是做批处理的，对于资源的开销比较高，但是并不要求高可用。
* 谷歌将两种业务类型混合部署，是的数据中心的资源利用率有一个本质的提升。
 
![](/commons/云原生/docker/image/2.1(2).png)

#### 1、Google Borg 简介特性

* 物理资源利用率高。通过在线、离线混部的方式，提高了资源利用率；同时其没有使用虚拟化技术，因此不需要模拟操作系统，所有的资源都是用来做计算的，也提高了资源利用率。
* 服务器共享，在进程级别做隔离。
* 应用高可用，故障恢复时间短。
* 调度策略灵活。
* 应用接入和使用方便，提供了完备的Job描述语言，服务发现，实时状态监控和诊断工具。

##### Google Borg 优势：
  * 对外隐藏底层资源管理和调度、故障处理等。
  * 实现应用的高可靠和高可用。
  * 足够弹性，支持应用跑在成千上万的机器上。

##### Google Borg 基本概念：
  * Workload：分为在线业务和离线业务。prod：在线任务，长期运行、对延时敏感、面向终端用户等，比如Gmail, Google Docs, Web Search服务等。non-prod∶：离线任务，也称为批处理任务（Batch），比如一些分布式计算服务等。
  * Cell：集群管理的概念，一个 Cell 上跑一个集群管理系统Borg。通过定义 Cell 可以让Borg 对服务器资源进行统一抽象，作为用户就无需知道自己的应用跑在哪台机器上，也不用关心资源分配、程序安装、依赖管理、健康检查及故障恢复等。
  * Job和Task：作业描述和调度的概念，用户以 Job 的形式提交应用部署请求。一个Job 包含一个或多个相同的Task，每个Task 运行相同的应用程序，Task 数量就是应用的副本数。每个 Job 可以定义属性、元信息和优先级，优先级涉及到抢占式调度过程。
  * Naming：服务发现的概念，Borg 的服务发现通过BNS ( Borg Name Service）来实现。50.jfoo.ubar.cc.borg .google.com 可表示在一个名为cc的Cell 中由用户 uBar 部署的一个名为 jFoo 的 Job 下的第50个Task。

#### 2、Borg 架构

首先有一个Cell的概念，即一个Borg集群，在一个集群内部有Borg Mater和Borg Slave，用户可以使用borg提供的命令或者浏览器向Borg Master提交一个任务，然后scheduler就会从Borg Matser中读取任务并按照一定的调度策略将其调度到Slave上，那么就会将Slave节点与Job绑定，在Slave中存在代理Boeglet会读取调度到当前节点任务并执行。

![](/commons/云原生/docker/image/2.1(3).png)
　　　　　　　　
* Borgmaster主进程∶
  * 处理客户端RPC请求，比如创建Job，查询Job等。
  * 维护系统组件和服务的状态，比如服务器、Task等。
  * 负责与Borglet通信。

* Scheduler进程∶
  * 调度策略（Worst Fit（资源利用率最低的节点）、Best Fit（刚好满足需求的节点，例如需要两个cpu，那么就找刚好有两个cpu的节点）、Hybrid（混合模式））

* 调度优化：
  * Score caching：当服务器或者任务的状态未发生变更或者变更很少时，直接采用缓存数据，避免重复计算。
  * Equivalence classes：调度同一Job下多个相同的Task只需计算一次。
  * Relaxed randomization∶ 引入一些随机性，即每次随机选择一些机器，只要符合需求的服务器数量达到一定值时，就可以停止计算，无需每次对Cell中所有服务器进行feasibility checking。

* Borglet：
  * Borglet是部署在所有服务器上的Agent，负责接收Borgmaster进程的指令。

#### 3、高可用与利用率

* 应用高可用：
  * prod 任务的要求就是高可用，如果资源不够时，会 kill non-prod 任务，被抢占的 non-prod 任务放回 pending queue，等待重新调度。
  * 多副本应用跨故障域部署。所谓故障域有大有小，比如相同机器、相同机架或相同电源插座等，一挂全挂。
  * 对于类似服务器或操作系统升级的维护操作，避免大量服务器同时进行。
  * 支持幂等性，支持客户端重复操作。
  * 当服务器状态变为不可用时，要控制重新调度任务的速率。因为 Borg 无法区分是节点故障还是出现了短暂的网络分区，如果是后者，静静地等待网络恢复更利于保障服务可用性。
  * 当某种“任务 @ 服务器”的组合出现故障时，下次重新调度时需避免这种组合再次出现，因为极大可能会再次出现相同故障。
  * 记录详细的内部信息，便于故障排查和分析。
  * 保障应用高可用的关键性设计原则∶无论何种原因，即使 Borgmaster 或者 Borglet 挂掉、失联，都不能杀掉正在运行的服务（Task）。

* 系统自身高可用
  * Borgmaster组件多副本设计。
  * 采用一些简单的和底层（low-level）的工具来部署Borg系统实例，避免引入过多的外部依赖。
  * 每个Cell的Borg均独立部署，避免不同Borg系统相互影响。

* 资源利用率：
  * 通过将在线任务（prod）和离线任务（non-prod，Batch）混合部署，空闲时，离线任务可以充分利用计算资源；繁忙时，在线任务通过抢占的方式保证优先得到执行，合理地利用资源。
  * 98%的服务器实现了混部。
  * 90%的服务器中跑了超过25个Task和4500个线程。
  * 在一个中等规模的Cell里，在线任务和离线任务独立部署比混合部署所需的服务器数量多出约 20%-30%。可以简单算一笔账，Google 的服务器数量在千万级别，按 20% 算也是百万级别，大概能省下的服务器采购费用就是百亿级别了，这还不包括省下的机房等基础设施和电费等费用。

#### 4、Brog调度原理
![](/commons/云原生/docker/image/2.1(4).png)
　　　　　　　　
隔离性：
  * 安全性隔离：
    * 早期采用Chrootjail，后期版本基于Namespace。

  * 性能隔离：
    * 采用基于Cgroup的容器技术实现。
    * 在线任务（prod）是延时敏感（latency-sensitive）型的，优先级高，而离线任务（non-prod，Batch）优先级低。
    * Borg通过不同优先级之间的抢占式调度来优先保障在线任务的性能，牺牲离线任务。
    * Borg将资源类型分成两类：可压榨的（compressible），CPU是可压榨资源，资源耗尽不会终止进程；不可压榨的（non-Compressible），内存是不可压榨资源，资源耗尽进程会被终止。


## 二、Kubernetes 基础架构

### （一）Kubernetes 概述

#### 1、什么是Kubernetes(K8S)
Kubernetes是谷歌开源的容器集群管理系统，是Google多年大规模容器管理技术Borg的开源版本，主要功能包括：
* 基于容器的应用部署、维护和滚动升级；
* 负载均衡和服务发现；
* 跨机器和跨地区的集群调度；
* 自动伸缩；
* 无状态服务和有状态服务；
* 插件机制保证扩展性

#### 2、命令式（Imperative）vs 声明式（Declarativey）
* 命令式系统关注“如何做”（在软件工程领域，命令式系统是写出解决某个问题、完成某个任务或者达到某个目标的明确步骤。此方法明确写出系统应该执行某指令，并且期待系统返回期望结果。）；
* 声明式系统关注“做什么”（在软件工程领域，声明式系统指程序代码描述系统应该做什么而不是怎么做。仅限于描述要达到什么目的，如何达到目的交给系统。）
* 声明式可以保证幂等性，因为状态固定，每次我要你做事，请给我返回相同结果；同时声明式是面向对象的，把一切抽象成对象。

#### 3、Kubernetes：声明式系统
Kubernetes的所有管理能力构建在对象抽象的基础上，核心对象包括：
* Node∶计算节点的抽象，用来描述计算节点的资源抽象、健康状态等。
* Namespace∶资源隔离的基本单位，可以简单理解为文件系统中的目录结构。
* Pod∶用来描述应用实例，包括镜像地址、资源需求等。Kubernetes中最核心的对象，也是打通应用和基础架构的秘密武器。
* Service∶服务如何将应用发布成服务，本质上是负载均衡和域名服务的声明。

### （二）Kubernetes架构

#### 1、架构简述
Kubernetes采用与Borg类似的架构，从节点上有master 和 worker节点，从组件上有 API Server、Controller、scheduler、etcd、kubelet。
![](/commons/云原生/docker/image/2.1(5).png)
如上图所示，是一个Kubernetes的Cluster，集群中分为管理节点 Mater Node 和工作节点 Worker Node，master 节点中有 API Server、scheduler、controllers，worker 节点中kubelet。

* API Server类似一个 Rest Server，即接收 rest 请求，无论是通过命令行还是通过浏览器的输入，最终都会以命令的形式被 API Server 接收，然后 API Server 经过鉴权认证后，会将命令存储到 etcd 数据库，etcd 可以在访问数据的时候添加一个 watch，一旦 watch 的数据有变化，etcd 会将其封装为一个事件通知给客户端，因此 etcd 还充当了类似与消息队列的角色，也就是说 API Server 会将请求推送给 etcd，etcd也会将对象的变更推送给 API Server。那么 API Server 可以接收客户端或者各个组件的请求外，还可以把对象变更推送出去，这样整个系统就可以联动起来了。也就是说其他的组件不互相通信，所有的组件都将请求发送给API Server，API Server 做认证、安全、校验后把数据推送到 etcd，同时 API Server 会将任何对象的变化推送到其他组件，通过消息的机制来驱动整个系统的变化。
在 worker 节点上有 kubelet，其会定时上报 worker 节点状态到 API Server，API Server会将状态存入 etcd 数据库，同时 scheduler 就能接收到这些变化的请求，也就是说 scheduler 是知道整个集群中 worker 节点的使用情况。
* 用户创建一个调度请求（创建 pod ）给 API Server，那么 API Server 会将请求发送 scheduler，scheduler会根据请求的内容（创建几个 pod，占用多少资源）以及 worker 节点的资源利用率选择合适的节点就行调度，也就是将 pod 和调度的节点通知 API Server，然后 API Server 把 pod 中的 node name 信息存储到 etcd 中。
kubelet 从 API Server 中获哪个 pod 和自己的节点有关，如果有关，就会进行任务处理，创建一个pod。
* 除了上面的流程外，还有一个 Controllers，其是对其他组件的一个管理，例如配置一个 Service 的域名配置和负载均衡等。

#### 2、主要组件
![](/commons/云原生/docker/image/2.1(6).png)

对上面的架构稍微展开一点看一下细节：
* API Server 实际上就是一个 Rest Server，在其中注册了一些对象的 handler，当要操作一个对象的时候，实际上是将其转化成了 Rest 的调用，并由 API Server 接收，其实 API Server 就类似于整个集群的网关，那么对于任何的网关来说，都有很多附加的功能，例如认证（知道你是谁，确认客户端是合法的）、鉴权（是否有操作的权限）、准入（请求是否合法）等。
* Controller Manager，其控制了一堆管理器，例如 Deployment 控制器、Replica Set 控制器、Nodelifecycle 控制器等，其是让整个集群的大脑，是让整个集群运行起来的核心。
* Kubelet：主要做两件事，上报节点的资源使用情况，维护 pod 的生命周期。
* proxy（worker node）：当需要发布一个服务（Service）的时候，需要由 proxy 来配置负载均衡。
* cAdvisor：在每一个 pod 里面，在容器世界里，都是使用 Container 的 RuntimeService 去起的应用，其都是使用标准的 CNI 的接口，都有统一的 Cgroup 的配置，Namespace 的配置，那么就使得容器的配置标准化，那就会让容器的监控更简单。因此在每一个 Kubernetes 中都有一个 cAdvisor 的组件，用来收集容器进程的一些健康状况，例如资源利用量（用了多少 CPU，用了多少内存等），这样就可以使用容器技术、以很低的成本来构建一套监控体系，包括对资源使用量的监控，健康状态的监控等，这样就使得整个云平台标准化了。

#### 3、Kubernets主节点（Master Node）
* API服务器（API Server）：这是Kubernetes控制面板中唯一带有用户可访问API以及用户可交互的组件。API服务器会暴露一个RESTful的Kubernetes API并使用JSON格式的清单文件（manifestAPIServer files)。
* 集群的数据存储（ClusterDataStore）：Kubernetes 使用“etcd”。这是一个强大的、稳定的、高可用的键值存储，被Kubernetes用于长久储存所有的API对象。
* 控制管理器（ControllerManager）：被称为“kube-controller manager”，它运行着所有处理集群日常任务的控制器。包括了节点控制器、副本控制器、端点（endpoint）控制器以及服务账户等。
* 调度器（Scheduler）：调度器会监控新建的pods（一组或一个容器）并将其分配给节点。
* 这四个是核心组件，如果这些组件不起来，整个集群是不能工作的。
  ![](/commons/云原生/docker/image/2.1(7).png)
#### 4、Kubernetes工作节点（Worker Node）
* Kubelet：负责调度到对应节点的Pod的生命周期管理，执行任务并将Pod状态报告给主节点的渠道，通过容器运行时（拉取镜像、启动和停止容器等）来运行这些容器。它还会定期执行被请求的容器的健康探测程序。
* Kube-proxy：它负责节点的网络，在主机上维护网络规则并执行连接转发。它还负责对正在服务的pods进行负载平衡。


### （四）Kubernetes组件概述

#### 1、etcd
![](/commons/云原生/docker/image/2.1(8).png)
* etcd是CoreOS基于Raft开发的分布式key-value存储，可用于服务发现、共享配置以及一致性保障（如数据库选主、分布式锁等）。
* etcd 是基本的key-value存储；监听机制；key的过期及续约机制，用于监控和服务发现；原子CAS和CAD，用于分布式锁和leader选举
* 查看etcd中的信息：这里说明一下，在 etcd 中 etcdctl 是其命令行工具，可以使用 etcdctl 命令来查看相关信息

```
# 设置别名
alias ks='kubectl -n kube-system'
# 从 kube-system 下获取 pod
ks get pod
# 通过 exec 进入 pod 内部
ks exec -it etcd-node1 sh
# 通过 etcdctl 获取 / 开头的 key，如果去掉 --keys-only 就能看基础信息
etcdctl--endpoints https://localhost:2379 --cert  /etc/kubernetes/pki/etcd/server.crt--key /etc/kubernetes/pki/etcd/server.key--cacert/etc/kubernetes/pki/etcd/ca.crt get--keys-only--prefix/
```

* 如果想监听对象变化，可以使用 watch 来进行监听，一旦命令执行，那么这个watch就会一直存在。

```etcdctl--endpoints https://localhost:2379--cert /etc/kubernetes/pki/etcd/server.crt--key /etc/kubernetes/pki/etcd/server.key--cacert/etc/kubernetes/pki/etcd/ca.crt watch--prefix /registry/services/specs/default/mynginx```

#### 2、APIServer
![](/commons/云原生/docker/image/2.1(9).png)

* Kube-APIServer是Kubernetes最重要的核心组件之一，主要提供以下功能：
  * 提供集群管理的RESTAPI接口，包括∶认证Authentication、 授权Authorization、准入Admission（Mutating&Valiating）。
  * 提供其他模块之间的数据交互和通信的枢纽（其他模块通过APIServer查询或修改数据，只有APIServer才直接操作etcd）。
  * APIServer提供etcd数据缓存以减少集群对etcd的访问。

APIServer展开：
![](/commons/云原生/docker/image/2.1(10).png)

#### 3、Controller Manager：
* Controller Manager是集群的大脑，是确保整个集群动起来的关键；
* 作用是确保 Kubernetes 遵循声明式系统规范，确保系统的真实状态（Actual State）与用户定义的期望状态（Desired State）一致；
* Controller Manager是多个控制器的组合，每个Controller事实上都是一个control loop，负责侦听其管控的对象，当对象发生变更时完成配置；
* Controller 配置失败通常会触发自动重试，整个集群会在控制器不断重试的机制下确保最终一致性（Eventual Consistency）。

控制器的工作流程：
![](/commons/云原生/docker/image/2.1(11).png)
Informer的内部机制:
![](/commons/云原生/docker/image/2.1(12).png)
控制器的协同工作原理：
![](/commons/云原生/docker/image/2.1(13).png)

#### 4、Scheduler：
![](/commons/云原生/docker/image/2.1(14).png)
* 特殊的Controller，工作原理与其他控制器无差别。
* Scheduler的特殊职责在于监控当前集群所有未调度的Pod，并且获取当前集群所有节点的健康状况和资源使用情况，为待调度Pod选择最佳计算节点，完成调度。
* 调度阶段分为：
  * Predict∶过滤不能满足业务需求的节点，如资源不足、端口冲突等。
  * Priority∶按既定要素将满足调度需求的节点评分，选择最佳节点。
  * Bind∶将计算节点与Pod绑定，完成调度。

#### 5、Kubelet：
![](/commons/云原生/docker/image/2.1(15).png)

* Kubernetes的初始化系统（init system）
* 从不同源获取Pod清单，并按需求启停Pod的核心组件：
* Pod 清单可从本地文件目录，给定的HTTPServer或Kube-APIServer等源头获取；
* Kubelet 将运行时，网络和存储抽象成了CRI，CNI,CSI。
* 负责汇报当前节点的资源信息和健康状态；
* 负责Pod的健康检查和状态汇报。


#### 6、Kube-Proxy
![](/commons/云原生/docker/image/2.1(16).png)
　　　　　　　　
  * 监控集群中用户发布的服务，并完成负载均衡配置。·  每个节点的Kube-Proxy都会配置相同的负载均衡策略，使得整个集群的服务发现建立在分布式负载均衡器之上，服务调用无需经过额外的网络跳转（Network Hop）。
  * 负载均衡配置基于不同插件实现：userspace、操作系统网络协议栈不同的Hooks点和插件（iptables、ipvs）


#### 7、推荐的Add-ons：
  * kube-dns∶负责为整个集群提供DNS服务；· Ingress Controller∶ 为服务提供外网入口；
  * MetricsServer∶提供资源监控；·  Dashboard∶提供GUI；
  * Fluentd-Elasticsearch∶提供集群日志采集、存储与查询。


### （五）K8S主要功能：

* K8s是用来对docker容器进行管理和编排的工具，其是一个基于docker构建的调度服务，提供资源调度、均衡容灾、服务注册、动态扩容等功能套件，其作用如下所示：
  * （1）数据卷：pod中容器之间数据共享，可以使用数据卷
  * （2）应用程序健康检查：容器内服务可能发生异常导致服务不可用，可以使用健康检查策略保证应用的健壮性。
  * （3）复制应用程序实例：控制器维护着pod的副本数量，保证一个pod或者一组同类的pod数量始终可用。
  * （4）弹性伸缩：根据设定的指标（CPU利用率等）动态的自动缩放pod数
  * （5）负载均衡：一组pod副本分配一个私有的集群IP地址，负载均衡转发请求到后端容器，在集群内布，其他pod可通过这个Cluster IP访问集群。
  * （6）滚动更新：更新服务不中断，一次更新一个pod，而不是同时删除整个服务
  * （7）服务编排：通过文件描述部署服务，使的程序部署更高效。
  * （8）资源监控：Node节点组件集成cAdvisor资源收集工具，可通过Heapster汇总整个集群节点资源数据，然后存储到InfluxDB时序数据库，再由Grafana展示
  * （9）提供认证和授权：支持属性访问控制、角色访问控制等认证授权策略。

K8S的架构图如下所示：
![](/commons/云原生/docker/image/2.1(17).png)　

* 从上图可以看到，K8S提供了：
  * web browsers提供可视化操作
  * kubectl来接收Docker镜像进行部署
  * scheduler进行任务调度
  * Controller进行请求控制
  * API Server进行请求的统一网关处理
  * etcd用于存储集群中的网络及状态信息
  * Kubelet来接收控制器的处理任务
  * Container Registry用来存储镜像仓库


## 二、K8S集群

### （一）集群概述

* 一个K8S集群包含一个master节点和一群node节点，Mater节点负责管理和控制，Node节点是工作负载节点，里面是具体的容器，容器中部署的是具体的服务。
* Mater节点包括API Server、Scheduler、Controller Manager、etcd。
  * API Server：整个集群的对外接口，供客户端和其他组件调用。
  * Scheduler：负责集群内部资源调度
  * Controller Manager：负责管理控制器。
  * etcd：用于保存集群中所有网络配置和对象状态信息。
* node节点包括Docker、kubelet、kube-proxy、Fluentd、kube-dns（可选）、pod

### （二）Mater节点

上面已经提到，Mater节点包括API Server、Scheduler、Controller Manager、etcd。

#### 1、API Server
API Server是整个集群的统一入口，各组件的协调者，以HTTP的形式对外提供服务，所有对象资源的增删改查和监听操作都交给API Server处理后再提交给etcd进行存储。

#### 2、Scheduler
Scheduler负责集群内部资源调度，其会根据调度算法为新创建的pod选择一个node节点，Scheduler在整个集群中起到了承上启下的重要功能，承上是指她负责接收Controller Manager创建的新的pod，为其安排一个node节点，启下指的是当为pod选定node节点后，目标Node上的kubelet服务进程会接管该pod。
![](/commons/云原生/docker/image/2.1(18).png)

这里就要提一下创建Pod的流程：

* （1）kubectl发送创建pod的请求，此时这个命令被apiserver拦截，把创建的pod存储到etcd的podQueue
* （2）Scheduler发起调用请求，此时这个命令被apiserver拦截，获取etcd中podQueue.NodeList，使用调度算法（调度算法：预选调度、优选策略）选择一个合适的node节点
* （3）把选择合适的pod、node存储到etcd中
* （4）node节点上的Kubelet进程，发送请求获取pod、node对应创建资源
* （5）此时node发现pod是本node需要创建的，kubelet就开始创建pod

#### 3、Controller Manager
每个资源都对应一个控制器（Kubernets Controller），其用来处理集群中常规的后台任务，而Controller Manager是用来负责管理控制器的。

K8S集群有以下控制器：

* （1）Replication Controller：保证Replication Controller中定义的副本数量与实际运行的pod数量一致。
* （2）Node Controller：管理维护Node，定期检查Node节点的健康状态，标识出失效和未失效的Node节点。
* （3）Namespace Controller：管理维护Namespace，定期清理无效的Namespace，包括Namespace下的API对象，例如pod和service等
* （4）Service Controller：管理维护Service，提供负载以及服务代理。
* （5）Endpoints Controller：管理维护Endpoints，即维护关联service和pod的对应关系，其对应关系通过Label来进行关联的
* （6）Service Account Controller：管理维护Service Account，为每个Namespace创建默认的Service Account，同时为Service Account创建Service Account Secret。
* （7）Persistent Volume Controller：持久化数据控制器，用来部署有状态服务
* （8）Deamon Set Controller：让每一个Node节点都运行相同的服务
* （9）Deployment Controller：无状态服务部署控制器
* （10）Job Controller：管理维护Job，为Job创建一次性任务Pod，保证完成Job指定完成的任务数目。
* （11）Pod Autoscaler Controller：实现pod的自动伸缩，定时获取监控数据，进行策略匹配，当满足条件时执行pod的伸缩动作。

#### 4、etcd
* etcd是一个第三方服务，分布式键值对存储系统，用于保存网络配置、集群状态等信息，例如service、pod等对象的信息。
* K8S中一共有两个服务需要用到etcd来协调和存储数据，分别是网络插件flannel和K8S本身，其中flannel使用etcd存储网络配置信息，K8S本身使用etcd存储各种对象的状态和元信息配置。

### （三）Node节点
上面提到，node节点包括Docker、kubelet、kube-proxy、Fluentd、kube-dns（可选）、pod等信息。
![](/commons/云原生/docker/image/2.1(19).png)

#### 1、kubelet
* kubelet是Mater在Node节点上的代理，每个Node节点都会启动一个kubelet进程，用来处理Mater节点下发到Node节点的任务，管理本机运行容器的生命周期，比如创建容器、Pod挂载数据卷、下载secret、获取容器和节点的状态等工作，kubelet将每个pod转换成一组容器。
* kubelet默认监听四个端口：10250、10255、10248、4194
* 10250端口：kubelet API的端口，也就是kubelet server与api server的通讯端口，定期请求apiserver获取自己所应当处理的任务，通过该端口可以访问和获取node资源及状态。
* 10248端口：健康检查的端口，通过访问该端口可以判断kubelet是否正常工作，可以通过 kubelet 的启动 参数 --healthz-port 和 --healthz-bind-address 来指定监听的地址和端口
* 4194端口：kubelet通过该端口可以获取到该节点的环境信息以及node上运行的容器状态等内容，访问 http://localhost:4194 可以看到 cAdvisor 的管理界面,通过 kubelet 的启动参 数 --cadvisor-port 可以指定启动的端口。
* 10255端口：提供了pod和node的信息，接口以只读形式暴露出去，访问该端口不需要认证和鉴权。

#### 2、kube-proxy
* 在Node节点上实现Pod网络代理，维护网络规则和四层负载均衡工作，kube-proxy本质上类似于一个反向代理，我们可以把每个节点上运行的kube-proxy看作是service的透明代理兼LB。
* kube-proxy监听apiserver中service与endpoints的信息，配置iptables规则，请求通过iptables直接转发给pod。
  ![](/commons/云原生/docker/image/2.1(20).png)
#### 3、docker
* 运行容器的引擎，pod内部运行的都是容器，这个容器是由Docker引擎创建的，Docker引擎是node节点的基础服务。

#### 4、pod
* pod是最小的部署单元，一个pod由一个或多个容器组成，pod中共享存储和网络，在同一个Docker主机上运行。pod内部可以运行一个或多个容器，一般情况下，为了便于管理，一个pod下只运行一个容器。

### （四）Pod
* pod就是一个容器，内部封装了docker容器，同时拥有自己的ip地址，也有用自己的HostName，Pod就像一个物理机一样，实际上Pod就是一个虚拟化的容器（进程），pod中运行的是一个或者多个容器。
  ![](/commons/云原生/docker/image/2.1(21).png)
* pod是一个大的容器，由K8S创建，pod内部的是docker容器，由Docker引擎创建。K8S不会直接管理容器，而是管理Pod。
* pod的作用是管理线上运行的应用程序，在通常情况下，在服务上线部署的时候，pod通常被用来部署一组相关的服务。而一个调用链上的服务就叫做一组相关的服务。但是实际生产上一般是一个Pod对应一个服务，不会在一个Pod上部署太多的服务。
* pod的具体结构如下图所示，一个Node中有很多pause容器，这些pause容器和pod是一一对应的，每个Pod里面运行着一个特殊的被称为Pause的容器，其他的容器则为业务容器，这些容器共享Pause容器的网络和存储，因此他们之间的通信更高效，在同一个pod里面的容器之间仅需要通过localhost就可以通信。
  ![](/commons/云原生/docker/image/2.1(22).png)
* K8S中的pause容器主要为每个业务容器提供以下功能，从而对各个Pod进行了隔离：
  * PID命名空间隔离：pod中不同的应用程序可以看到其他应用程序的进程ID
  * 网络命名空间隔离：Pod中多个容器能够访问同一个IP和端口范围
  * IPC命名空间隔离：Pod中多个容器能够使用System VIPC或POSIX消息队列进行通信
  * UTS命名空间隔离：pod中多个容器共享一个主机名和挂在卷
  * Pod中各个容器可以访问在Pod级别定义的Volumes
* 一个Pod创建的过程：首先kubelet会先创建一个pod，然后立马会创建一个pause容器，pause容器是默认创建的，然后再创建内部其他的业务容器。


## 三、核心组件及原理
### 1、RC控制器（ReplicationController）

用来确保容器应用的副本数始终与用户定义的副本数一致，如果有副本由于异常退出，Replication Pod会自动创建新的Pod来替代，而如果出现多余的pod，控制器也会自动将其回收。

在新版的K8S中，建议使用ReplicaSet来取代Replication Controller

### 2、RS控制器（ReplicaSet）

ReplicaSet和Replication Contreoller并没有本质上的区别，ReplicaSet支持集合式的选择器。

虽然ReplicaSet可以独立使用，但一般情况下还是建议使用Deployment来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题。

RC和RS的区别是RC只支持单个标签选择器，不支持复合标签选择器；而RS同时支持单个和复合选择器。

### 3、label（标签）

label用于区分对象，例如区分是service还是pod，以键值对的形式存在，每个对象可以有多个标签，可以通过标签关联对象。

label是replication Controller和Service运行的基础，二者是通过label来进行关联Node上运行的Pod。我们可以通过给指定的资源对象捆绑一个或多个不同的Label来实现多维度资源分配管理功能，一些常用的Label如下所示：
```
版本标签："release":"stable","release":"canary"......
环境标签："environment":"dev","environment":"qa","environment":"production"
架构标签："tier":"frontend","tier":"backend","tier":"middleware"
分区标签："partition":"customerA","partition":"customerB"
质量管控标签："track":"daily","track":"weekly"
```
label就类似与标签，给某个对象定义了一个label就相当于给对象定义了一个标签。

如果多个pod拥有相同的标签，就说明这是一组pod。

### 4、selector

标签选择器是K8S非常重要的一环，其用来查询和筛选某些拥有具体标签的对象，K8S也是使用这种方式进行对象的查询。

Label Selector在K8S中的应用有以下几个场景：

Kube-Controller进程通过资源对象RC上定义的Label Selector来筛选要监控的Pod的副本数量，从而实现副本数量与用户定义的副本数量保持一致。

Kube-proxy进程通过Service的Label Selector来筛选对应的Pod，自动建立起每个Service到对应Pod的请求链路表，从而实现Service的负载均衡。

Kube-Scheduler通过对某些Pod的自定义Label，并且在pod定义文件中使用Node Selector这种标签的调度策略，从而实现了Pod定向调度的特性。

例如上面Label的例子中，如果服务中既有环境Label又有版本Label，使用RC只能对同一个版本标签或者同一个环境标签的Pod进行选择，不能同时对版本和环境两个维度进行筛选，而RS可以对两个维度同时进行筛选。

### 5、Deployment

RS虽然可以控制副本的数量，但是单独的RS部署却不能滚动更新。因此衍生了Deployment组件，其支持滚动更新，其会先创建新版本的pod容器，然后再删除老旧版本的pod容器。

滚动发布：滚动发布有金丝雀发布和灰度发布，该种发布一般是以25%的模式进行发布，也就是先删除25%旧版本，在部署对应数量的新版本Pod，然后再删除25%旧版本，这样以此滚动更新。

因此实际生产中一般都是用RS和Deployment的组合来进行发布服务。

总体来说，Deployment是用来管理RS，RS来管理Pod，deployment支持动态更新，也就是在更新时动态的创建一个新的RS，然后由新RS创建新版本的Pod，然后将旧版本RS中的Pod进行删除。如果发生回滚，就是一个逆向操作，产生一个旧版本的RS，用来生成旧版本的Pod。

在滚动发布过程中，对于流量是转发到新版本还是老版本的Pod中，是由商城的Service进行转发的。

Deployment为Pod和ReplicaSet提供了一个声明式定义方法，典型的应用场景：

定义Deployment来创建Pod和ReplicaSet

滚动升级和回滚应用

扩容和缩容

暂停和继续Deployment
![](/commons/云原生/docker/image/2.1(23).png)
　　　　　　

### 6、HPA（HorizontalPodAutoScale）

HPA仅适用于Deployment和ReplicaSet，在V1.0版本中，仅支持根据pod的CPU利用率进行扩容缩容，在新版本中，支持根据内存和用户自定义的metric进行扩容缩容。说的通俗一点，就是在流量突然增大，可以自动扩容，流量降下后，可以自动缩容。

### 7、StatefullSet

deployment和StatefullSet都是用来进行服务部署的，但是这两个组件各自使用的场景不一样，deployment是用来部署无状态服务的，而StatefullSet是用来部署有状态服务的。

这里说明一下有状态服务和无状态服务，有状态服务指的是需要进行实时的数据更新和存储的服务，如果将某个服务抽离，再加入进来就没办法进行正常工作，例如mysql、redis等；无状态服务指的是没有对应数据进行更新和存储的服务，如果将某个服务抽离，再加入进来依然可以提供服务，我们常用的业务服务一般都是无状态服务，而docker业主要是为了无状态服务提供部署的。

StatefullSet应用场景包括：

（1）稳定的持久化存储，即pod重新调度后仍然可以访问相同的持久化数据，基于PVC实现

（2）稳定的网络标志，即pod重新调度后，podName和HostName不变，基于Headless Service来实现

（3）有序部署&有序扩展，即pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次进行，也就是说，从0到N-1，在下一个pod运行前，所有之前的pod都处于running和ready状态。基于init contains实现。

（4）有序收缩&有序删除，即从N-1到0进行回收或删除

总体来说，Pod是可能随时删除或者新增的，一个pod也是有自己的网络和存储的，对于例如Mysql这类的Pod，可能不能发生网络和存储上的变化，StatefullSet就是为了解决这个问题而产生的。

### 8、DaemonSet

DaemonSet确保全部Node上运行一个Pod副本，当有Node加入集群时，也会为他们新增一个pod，当有Node从集群中被移除时，这些pod也会被回收，删除DaemonSet将会删除其创建的所有pod。最典型的场景就是每个Pod里面都有服务在运行，需要收集服务运行日志，但是Pod是由K8S自动创建或删除的，因此需要使用DaemonSet来设定在每一个Pod中进行日志收集。

DaemonSet的一些典型用法：

（1）运行集群存储Daemon，例如在每个Node上运行glustered、ceph

（2）在每个Node上运行日志收集Daemon，例如fluentd、logstash

（3）在每个Node上运行监控Daemon，例如Prometheus Node Exporter

Job负责批处理任务，即仅执行一次的任务，他保证批处理任务的一个或多个pod成功结束。

### 9、Volume

数据卷，共享pod中容器使用的数据。


## SeeMore
* https://www.cnblogs.com/liconglong/p/15044124.html#_label1
* https://redhatxl.github.io/cloud-native/develop/08-client-go%E4%B9%8BInformer%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/#_2
*  [k8s源码走读](https://cncamp.notion.site/cncamp/kubernetes-8a9d48ee26284b3c8ddf9de4c62ea895)
