---
title: 2.4 kube-scheduler
date: 2023-02-19
categories: [笔记, '云原生', 'k8s']
tags: [云原生]
---

# kube-scheduler
kube-scheduler负责分配调度Pod到集群内的节点上/它监听kube-apiserver,查询还未分配 Node的Pod,然后根据调度策略为这些Pod分配节点(更新Pod的NodeName字段)。 

### 调度器需要充分考虑诸多的因素：
* 公平调度；
* 资源高效利用；
* QoS;
* affinity 和 anti-affinity;
* 数据本地化(data locality);
* 内部负载干扰(inter-workload interference);
* deadlines

### kube-scheduler调度分为两个阶段, predicate 和 priority:
* predicate:过滤不符合条件的节点；
* priority:优先级排序，选择优先级最高的节点。

#### predicate策略
* PodFitsHostPorts:检查是否有 Host Ports 冲突。
* PodFitsPorts:同 PodFitsHostPorts0
* PodFitsResources:检查Node的资源是否充足/包括允许的Pod数量、CPU、内存、GPU个数以及其他的OpaquelntResources。
* MatchlnterPodAffinity:检查是否匹配Pod的亲和性要求。
* NoDiskConflict:检查是否存在 Volume 冲突/ 仅限于 GCEPD、AWSEBS、Ceph RBD 以及 iSCSIo
* PodToleratesNodeTaints:检查 Pod 是否容忍 Node Taintso
* CheckNodeMemoryPressure:检查 Pod 是否可以调度到 Memorypressure 的节点上。
* CheckNodeDiskPressure:检查Pod是否可以调度到DiskPressure的节点上。
* NoVolumeNodeConflict:检查节点是否满足Pod所引用的Volume的条件。
* 还有很多其他策略’你也可以编写自己的策略。

![](/commons/云原生/docker/image/2.4(1).png)

#### priority策略
* SelectorSpreadPriority:优先减少节点上属于同一个 Service 或 Replication Controller 的 Pod 数量。
* InterPodAffinityPriority:优先将Pod调度到相同的拓扑上（如同一个节点、 Rack、Zone 等）。
* LeastRequestedPriority:优先调度到请求资源少的节点上。
* BalancedResourceAllocation:优先平衡各节点的资源使用。
* NodePreferAvoidPodsPriority: alpha.kubernetes.io/preferAvoidPods 字段 判断'权重为10000,避免其他优先级策略的影响。
* NodeAffinityPriority :优先调度到匹配 Node Affinity 的节点上。
* TaintTolerationPriority :优先调度到匹配 TaintToleration 的节点上。
* Servicespreading Priority:尽量将同一个service的Pod分布到不同节点上/已经 被SelectorSpreadPriority#代（默认未使用）。
* EqualPriority:将所有节点的优先级设置为1 （默认未使用）。
* ImageLocalityPriority:尽量将使用大镜像的容器调度到已经下拉了该镜像的节 点上（默认未使用）。
* MostRequestedPriority:尽量调度到已经使用过的Node上,特别适用于cluster- autoscaler （默认未使用）。*  CPU

### 资源需求
* CPU
  * requests
    * Kubernetes调度Pod时/会判断当前节点正在运行的Pod的CPU Request的总和,再 加上当前调度Pod的CPU request,计算其是否超过节点的CPU的可分配资源
  * Limits
    * 配置cgroup以限制资源上限
* 内存
  * requests
    * 判断节点的剩余内存是否满足Pod的内存请求量,以确定是否可以将Pod调度到该节点
  * Limits
    * 配置cgroup以限制资源上限
* 容器磁盘资源      
  * 容器临时存储(ephemeral storage)包含日志和可写层数据，可以通过定义Pod Spec中的 limits.ephemeral-storage 和 requests.ephemeral-storage 来 申请。
  * Pod调度完成后，计算节点对临时存储的限制不是基于CGroup的，而是由kubelet定时获取容器的日志和容器可写层的磁盘使用情况，如果超过限制，则会对Pod进行驱逐。

* Init Container的资源需求
  * 当kube-scheduler调度带有多个init容器的Pod时/只计算cpu.request最多的init容器/而不是计 算所有的init容器总和。
  * 由于多个init容器按顺序执行，并且执行完成立即退出,所以申请最多的资源init容器中的所需资源, 即可满足所有init容器需求。
  *  kube-scheduler在计算该节点被占用的资源时, init容器的资源依然会被纳入计算。因为init容器在 特定情况下可能会被再次执行，比如由于更换镜像而引起Sandbox重建时。
     

### 把Pod调度到指定Node上

* 可以通过 nodeselector、nodeAffinity、podAffinity 以 及Taints和tolerations等来将Pod调度到需要的Node上
* 也可以通过设置nodeName参数,将Pod调度到指定 node节点上。比如使用nodeselector,首先给Node加上标签： kubectl label nodes <your-node-name> disktype=ssd 接着，指定该Pod只想运行在带有disktype=ssd标签的Node上

#### nodeselector
首先给 node 打标签: kubectl label nodes node-01 disktype=ssd

然后在 daemonset 中指定 nodeSelector 为 disktype=ssd:

```
spec:
nodeSelector:
disktype: ssd
```
![](/commons/云原生/docker/image/2.4(2).png)

#### NodeAffinity 
目前支持两种：requiredDuringSchedulinglgnoredDuringExecution 和 preferredDuringSchedulinglgnoredDuringExecution,分别代表必须满足条件和优选条件。
比如下面的例子代表调度到包含标签Kubernetes.io/e2e-az-name并且值为e2e-az1或e2e-az2的 Node 上/ 并且优选还带有标签 another-node-label-key=another-node-label-value 的 Node0
![](/commons/云原生/docker/image/2.4(3).png)
#### podAffinity
podAffinity基于Pod的标签来选择Node,仅调度至满足条件Pod所在的Node上,支持 podAffinity和podAntiAffinity。这个功能比较绕/以下面的例子为例：
如果一个“Node所在Zone中包含至少一个带有security=S1标签且运行中的Pod”,那么可以调度 到该Node,不调度到〃包含至少一个带有security=S2标签且运行中Pod〃的Node上。

![](/commons/云原生/docker/image/2.4(4).png)


#### Taints和Tolerations
用于保证Pod不被调度到不合适的Node.其中Taint应用于Node, 而Toleration则应用于Pod

* 目前支持的Taint类型:
  * NoSchedule:新的Pod不调度到该Node上,不影响正在运行的Pod;
  * PreferNoSchedule: soft版的NoSchedule,尽量不调度到该Node上;
  * NoExecute:新的Pod不调度到该Node上,并且删除(evict)已在运行的Pod。Pod可以增加一个时间(tolerationSeconds)。

> 然而当Pod的Tolerations匹配Node的所有Taints的时候可以调度到该Node;当Pod是已经运行的时候,也不会被删除(evicted)。另外对于NoExecute,如果Pod增加了一个 tolerationSeconds,则会在该时间之后才删除Pod

### 调度策略

#### 优先级调度

```
从v1.8开始kube-scheduler支持定义Pod的优先级,从而保证高优先级的Pod优先调度。开启方法为：
apiserver 配置--feature-gates=PodPriority=true 和--runtime-
config=scheduling.k8s.io/v1alpha1=true 
kube-scheduler 配置--feature-gates=PodPriority=truePriorityclass
 
在指定Pod的优先级之前需要先定义一个Priorityclass （非namespace资源）/如:
apiVersion: vl
kind: Priorityclass
metadata:
name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."为 pod 设置 priority
apiVersion: v1
kind: Pod
```

#### 多调度器
如果默认的调度器不满足要求, 还可以部署自定义调度器, 并且在整个集群可以同时运行多个调度器实例, <br>
通过 podSpec.ScheduleName 来选择使用哪一个调度器(默认用内置) <br>
比如离线作业调度器/批处理调度器等满足不同场景

### 思考

Kubernetes集群一般是通用集群，可被所有用户共享，用户无需关心计算节点细节。 但往往某些自带计算资源的客户要求：
* 带着计算资源加入Kubernetes集群；
* 要求资源隔离。

#### 默认调度器 8000 个 pod 需要多久?
2分钟

#### 实现方案：
* 将要隔离的计算节点打上Taints;
* 在用户创建创建Pod时,定义tolerations来指定要调度到node taints。

#### 该方案有漏洞吗？如何堵住？

* 其他用户如果可以get nodes或者pods,可以看到taints信息，也可以用相同的tolerations占用资源。
* 不让用户get node detail?
* 不让用户get别人的pod detail?
* 企业内部，也可以通过规范管理，通过统计数据看谁占用了哪些node;
* 数据平面上的隔离还需要其他方案配合。

#### 来自生产系统的经验
* 用户会忘记打tolerance,导致pod无法调度/ pending;
* 新员工常犯的错误，通过聊天机器人的Q&A解决；
* 其他用户会get node detail,查到Taints,偷用资源。
* 通过dashboard,能看到哪些用户的什么应用跑在哪些节点上;
* 对于违规用户，批评教育为主。


### 生产经验
![](/commons/云原生/docker/image/2.4(7).png)
