---
title: 2.9 CSI
date: 2023-03-15
categories: [笔记, '云原生', 'k8s']
tags: [云原生]
---

# CSI

* 除外挂存储卷外,容器启动后/运行时所需文件系统性能直接影响容器性能；
* 早期的Docker采用Device Mapper作为容器运行时存储驱动，因为OverlayFS尚未合并进 Kernel;
* 目前Docker和containerd都默认以OverlayFS作为运行时存储驱动；
*  OverlayFS目前已经有非常好的性能,与 DeviceMapper相比优20%,与操作主机文件 性能几乎一致。
![](/commons/云原生/docker/image/2.9(1).png)

Kubernetes支持以插件的形式来实现对不同存储的支持和扩展，这些扩展基于如下三种方式:

### in-tree 插件
Kubernetes社区已不再接受新的in-tree存储插件/新的存储必须通过out-of-tree插件进行支持

### out-of-tree FlexVolume插件
FlexVolume 是指 Kubernetes 通过调用计算节点的本地可执行文件与存储插件进行交互<br>
FlexVolume插件需要宿主机 用root权限来安装插件驱动FlexVolume存储驱动需要宿主机安装attach, mount等工具/也需要具有root访问权限。

### out-of-tree  CSI插件
*  CSI通过RPC与存储驱动进行交互。
* 在设计CSI的时候/ Kubernetes对CSI存储驱动的打包和部署要求很少/主要定义了 Kubernetes的两个相关 模块：
  * kube-controller-manager :
    * kube-controller-manager模块用于感知CSI驱动存在。
    * Kubernetes的主控模块通过Unix domain socket （而不是CSI驱动）或者其他方式进行直接地 交互。
    * Kubernetes的主控模块只与Kubernetes相关的API进行交互。
    * 因此CSI驱动若有依赖于Kubernetes API的操作，例如卷的创建、卷的attach.卷的快照等， 需要在CSI驱动里面通过Kubernetes的API,来触发相关的CSI操作。
  * kubelet:
    * kubelet模块用于与CSI驱动进行交互。
    * kubelet 通过 Unix domain socket 向 CSI 驱动发起 CSI 调用（如 NodeStageVolume、 NodePublishVolume 等）/ 再发起 mount 卷和 umount 卷。
    * kubelet通过插件注册机制发现CSI驱动及用于和CSI驱动交互的Unix Domain Socket。
    * 所有部署在Kubernetes集群中的CSI驱动都要通过kubelet的插件注册机制来注册自己。CSI 的驱动一般包含 extemal-attacher、external-provisionerx external-resizer, external- snapshotter. node-driver-register、CSI driver等模块'可以根据实际的存储类型和需求进行不同 方式的部署。


![](/commons/云原生/docker/image/2.9(2).jpeg)

## CSI驱动
![](/commons/云原生/docker/image/2.9(3).jpeg)

### emptyDir (临时存储)

常见的临时存储主要就是emptyDir卷。<br>
emptyDir是一种经常被用户使用的卷类型，顾名思义, "卷〃最初是空的。当Pod从节点上删除时, emptyDir卷中的数据也会被永久删除。但当Pod的容器因为某些原因退出再重启时, emptyDir卷内 的数据并不会丢失。
默认情况下，emptyDir卷存储在支持该节点所使用的存储介质上，可以是本地磁盘或网络存储。 emptyDir也可以通过将emptyDir.medium字段设置为“Memory”来通知Kubernetes为容器安装 tmpfs,此时数据被存储在内存中，速度相对于本地存储和网络存储快很多。但是在节点重启的时候， 内存数据会被清除；而如果存在磁盘上，则重启后数据依然存在。另外，使用tmpfs的内存也会计入 容器的使用内存总量中，受系统的Cgroup限制。
emptyDir设计的初衷主要是给应用充当缓存空间，或者存储中间数据，用于快速恢复。然而，这并不 是说满足以上需求的用户都被推荐使用emptyDir,我们要根据用户业务的实际特点来判断是否使用 emptyDir。因为emptyDir的空间位于系统根盘，被所有容器共享，所以在磁盘的使用率较高时会触 发Pod的eviction操伝 从而影响业务的稳定。

> 注意
> * emptyDir 需要控制 size limit, 否则无限扩张的应用会撑爆磁盘导致主机不可用, 进而导致大规模集群故障
> * emptyDir size limit 生效后, kubelet 会定期对容器目录执行 du 操作, 会导致些许的性能影响 
> * size limit 达到以后, pod 会被驱逐, 原 pod 的日志配置等信息会消失


### hostPath (半持久存储)
常见的半持久化存储主要是hostPath卷。hostPath卷能将主机节点文件系统上的文件或目录挂载到指 定Pod中。对普通用户而言一般不需要这样的卷，但是对很多需要获取节点系统信息的Pod而言，却 是非常必要的。

例如, hostPath的用法举例如下：
* 某个Pod需要获取节点上所有Pod的Log,可以通过hostPath访问所有Pod的stdout输出存储目录/例如/var/Log/pods路径。
* 某个Pod需要统计系统相关的信息, 可以通过hostPath访问系统的/proc目录。

使用hostPath的时候,除设置必需的path属性外,用户还可以有选择性地为hostPath卷指定类型, 支持类型包含目录、字符设备、块设备等。

> 注意
> * 使用同一个目录的Pod可能会由于调度到不同的节点，导致目录中的内容有所不同。
> * Kubernetes在调度时无法顾及由hostPath使用的资源。
> * Pod被删除后，如果没有特别处理，那么hostPath上写的数据会遗留到节点上,占用磁盘空间。

### StorageClass/PV/PVC (持久化存储)
#### StorageClass
用于指示存储的类型，不同的存储类型可以通过不同的StorageClass来为用户提供服务。<br>
StorageClass主要包含存储插件provisioner卷的创建和mount参数等字段。<br>

![](/commons/云原生/docker/image/2.9(4).jpeg)

#### PVC 
由用户创建，代表用户对存储需求的声明，主要包含需要的存储大小、存储卷的访问模式、 StroageClass等类型 其中存储卷的访问模式必须与存储的类型一致 <br>
RWO ReadWriteOnce 该卷只能在一个节点上被mount,属性为可读可写 <br>
ROX ReadOnlyMany 该卷可以在不同的节点上被mount,属性为只读 <br>
RWX ReadWriteMany 该卷可以在不同的节点上被mount,属性为可读可写 <br>

#### PV
由集群管理员提前创建/或者根据PVC的申请需求动态地创建/它代表系统后端的真实的存储空间, 可以称之为卷空间。<br>

#### 生产环境实践经验
不同介质类型的磁盘，需要设置不同的StorageClass,以便让用户做区分。StorageClass需要设置磁盘介质的类型，以便用户了解该类存储的属性。<br>
在本地存储的PV静态部署模式下，每个物理磁盘都尽量只创建一个PV,而不是划分为多个分区来提供多个本地存储PV,避免在使用时分区之间的I/O干扰。<br>
本地存储需要配合磁盘检测来使用。当集群部署规模化后，每个集群的本地存储PV可能会超过几万个，如磁盘损坏 将是频发事件。此时，需要在检测到磁盘损坏、丢盘等问题后，对节点的磁盘和相应的本地存储PV进行特定的处理, 例如触发告警、自动cordon节点、自动通知用户等。<br>
对于提供本地存储节点的磁盘管理，需要做到灵活管理和自动化。节点磁盘的信息可以归一、集中化管理。在 local-volume-provisioner中增加部署逻辑，当容器运行起来时,拉取该节点需要提供本地存储的磁盘信息，例如 磁盘的设备路径，以Filesystem或Block的模式提供本地存储，或者是否需要加入某个LVM的虚拟组(VG)等。 local-volume-provisioner根据获取的磁盘信息对磁盘进行格式化，或者加入到某个VG,从而形成对本地存储支 持的自动化闭环。<br>


### 独占的 Local Volume
![](/commons/云原生/docker/image/2.9(5).jpeg)

### Dynamic Local Volume

#### 什么是Dynamic Local Volume
csi驱动需要汇报节点上相关存储的资源信息，以便用于调度 , 但是机器的厂家不同/汇报方式也不同。<br>
例如，有的厂家的机器节点上具有NVMe、SSD、HDD等多种存储介质，希望将这些存储介质分别进行汇报。<br>
这种需求有别于其他存储类型的CSI驱动对接口的需求，因此如何汇报节点的存储信息，以及如何让节点的存储信息应用于调度，目前并没有形成统一的意见。<br>
集群管理员可以基于节点存储的实际情况对开源CSI驱动和调度进行一些代码修改/再进行部署和使用<br>

#### Dynamic Local Volume的挂载流程
![](/commons/云原生/docker/image/2.9(6).jpeg)

#### Dynamic Local Volume的挑战
如果将磁盘空间作为一个存储池（例如LVM）来动态分配，那么在分配出来的逻辑卷空间的使用上, 可能会受到其他逻辑卷的I/O干扰，因为底层的物理卷可能是同一个。
如果PV后端的磁盘空间是一块独立的物理磁盘，则I/O。就不会受到干扰。

## ROOK
TODO 待补充

## 对比(数据应该如何保存)
