---
title: vllm
date: 2025-04-01
categories: [笔记, LLM, 推理框架]
tags: [LLM]
---

# vllm

vLLM 是一个高效的开源大语言模型（LLM）推理和服务系统​, 2025年3月的最新版本是0.7.5


### 架构概述 
1. 入口点（Entrypoints）
vLLM 提供了多个入口点用于与系统交互
LLM 类：​这是用于离线推理的主要 Python 接口，允许直接与模型交互，无需单独的推理服务器。​​
兼容 OpenAI 的 API 服务器：​vLLM 提供了一个与 OpenAI API 兼容的服务器，可通过命令 vllm serve <model> 启动，方便在线服务和集成。​

2. LLM 引擎（LLM Engine）
LLMEngine：​vLLM 的核心组件，负责接收客户端请求并生成模型输出。其功能包括输入处理、调度、模型执行（可能跨多个主机和/或 GPU 分布式执行）以及输出处理。​
AsyncLLMEngine：​LLMEngine 的异步封装，利用 asyncio 创建后台循环，持续处理传入请求，适用于需要处理多个并发请求并向客户端流式传输输出的在线服务场景。​

3. 工作进程（Worker）
每个工作进程负责运行模型推理，通常一个进程控制一个加速设备（如 GPU）。在使用张量并行和流水线并行时，可能会有多个工作进程协同工作。​

4. 模型运行器（Model Runner）
每个工作进程包含一个模型运行器对象，负责加载和执行模型，包括准备输入张量和捕获 CUDA 图等任务。​

5. 模型（Model）
模型运行器包含一个实际的 torch.nn.Module 实例，即具体的模型对象。vLLM 支持多种流行的开源模型，每种模型都有其特定的初始化逻辑。​

6. 类层次结构（Class Hierarchy）
![类层次结构](/commons/LLM/推理框架/1.png)

### vLLM 的类层次结构设计强调以下几点：​

可扩展性：​所有类都接受包含必要信息的配置对象（VllmConfig），便于在系统中传递和访问配置，支持快速添加新功能。​  
统一性：​通过统一模型类的构造函数接口，简化了模型的创建和初始化过程，方便支持多种模型类型和组合。​  
初始化时的分片和量化：​在模型初始化期间进行权重的分片和量化，避免了加载完整模型后再进行处理的高内存开销，提高了对大模型的支持能力。​  

总体而言，vLLM 的架构设计旨在提供高效、灵活且可扩展的 LLM 推理和服务能力。 

### vLLM 具备卓越的性能：
* 最先进的服务吞吐量
* 高效管理注意力键值存储，采用 PagedAttention 技术 
* 连续批处理 以优化请求管理 
* CUDA/HIP 图加速，实现 超快模型执行
* 多种量化方案：GPTQ、AWQ、INT4、INT8、FP8 
* 优化的 CUDA 内核，集成 FlashAttention 和 FlashInfer
* 支持推测解码（Speculative Decoding）
* 支持分块预填充（Chunked Prefill）

### vLLM 灵活且易用，具备以下特点：
 ✅ 无缝集成 Hugging Face 模型  
 ✅ 支持多种解码算法，如 并行采样、束搜索（beam search）等  
 ✅ 支持张量并行（Tensor Parallelism）和流水线并行（Pipeline Parallelism），实现分布式推理  
 ✅ 支持流式输出（Streaming Output）  
 ✅ 提供 OpenAI 兼容 API 服务器  
 ✅ 支持多种硬件：  
     NVIDIA GPU   
     AMD CPU/GPU   
     Intel CPU/GPU   
     PowerPC CPU   
     TPU   
     AWS Neuron  
 ✅ 前缀缓存（Prefix Caching）支持    
 ✅ 支持多 LoRA（Multi-LoRA）  

### 支持的模型类型：
 ✅ 主流 Transformer 结构 LLM（如 Llama）   
 ✅ 专家混合模型（MoE LLMs）（如 Mixtral、Deepseek-V2、Deepseek-V3）   
 ✅ 嵌入模型（Embedding Models）（如 E5-Mistral）   
 ✅ 多模态 LLMs（如 LLaVA）   

### 核心功能
1. Integration with Hugging Face（与 Hugging Face 集成）   
✅ 作用：   
 vLLM 可以直接加载 Hugging Face（HF）模型，无需额外转换格式。    
 兼容 HF Transformers 库，支持 AutoModelForCausalLM 等模型接口。   
示例：你可以使用 from_pretrained() 方法加载 HF 模型到 vLLM 进行推理。    
🔹 适用场景：    
 你在 HF 上找到一个开源模型（如 LLaMA、Mistral），可以 无缝加载到 vLLM 进行高效推理。     

2. vLLM’s Plugin System（vLLM 插件系统）    
✅ 作用：    
支持用户扩展功能，无需修改 vLLM 源码。     
 允许开发者编写 自定义调度策略、数据处理逻辑或新推理模式。     
🔹 适用场景：    
 你希望在 vLLM 中添加 自定义的 KV 缓存策略 或 自定义的 LoRA 适配层，可以通过插件系统实现。     

3. vLLM Paged Attention（vLLM 分页注意力）    
✅ 作用：    
优化大模型推理时的注意力机制，避免 显存占满（OOM） 问题。     
原理：将 KV 缓存拆分成小的“页面”（类似操作系统的分页机制），按需分配显存，而不是一次性分配所有注意力权重。     
比标准注意力更节省显存，特别适用于 长上下文推理。     
🔹 适用场景：
 你要 处理长文本输入（如 10K+ tokens），分页注意力可以显著减少显存占用，提高吞吐量。     

4. Multi-Modal Data Processing（多模态数据处理）    
✅ 作用：    
 vLLM 未来支持 多模态模型（如图文、音频、视频+文本）。     
 允许加载 视觉-语言模型（VLMs），例如 LLaVA、BLIP-2 等。     
🔹 适用场景：    
 你想运行一个 能理解图片 + 文字的模型（比如让 GPT-4V 这样的模型支持高效推理）。     

5. Automatic Prefix Caching（自动前缀缓存）    
✅ 作用：    
加速共享前缀的推理请求，减少重复计算。     
 例如多个请求的开头都是 "As an AI model, I think..."，vLLM 只计算一次这个部分，后续请求复用缓存，加速推理。     
🔹 适用场景：    
 你在 批量生成对话，多个请求有 相同的系统提示词（system prompt），前缀缓存能提升性能。     

6. Python Multiprocessing（Python 多进程支持）     
✅ 作用：    
vLLM 可以利用 Python 的多进程能力，更高效地调度任务，避免 GIL（全局解释器锁）影响。     
提升多线程/多进程环境下的吞吐量，更适合大规模 API 部署。     
🔹 适用场景：    
 你要 在 Flask/FastAPI 这样的服务器上部署 vLLM，多进程可以优化并发能力。    

