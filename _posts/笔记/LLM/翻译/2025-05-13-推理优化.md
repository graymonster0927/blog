---
title: 掌握LLM技巧:推理优化
date: 2025-05-13
categories: [笔记, LLM, 翻译]
tags: [LLM]
---

# 掌握 LLM 技巧:推理优化

> 原内容 [@https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/](Mastering LLM Techniques: Inference Optimization)

将多个 Transformer 层堆叠起来以构建大型模型，能够提升准确率、实现少样本学习能力，甚至在多种语言任务上展现出接近人类的涌现能力。这些基础模型的训练成本极高，而且在推理阶段也可能非常耗费内存和计算资源（这是一种持续性的开销）。目前最流行的大型语言模型（LLMs）参数量可达数十亿甚至上千亿。根据具体使用场景，模型还可能需要处理很长的输入（或上下文），这也会增加开销。例如，检索增强生成（RAG）流程通常需要将大量信息输入模型，从而显著增加了 LLM 所需的处理工作量。

本文将探讨 LLM 推理阶段面临的最紧迫挑战，并提出一些可行的解决方案。读者应具备基本的 Transformer 架构和注意力机制知识。了解 LLM 推理中的复杂细节至关重要，我们将在下一部分进行详细讲解。

## 理解 LLM 推理

大多数流行的仅解码器（decoder-only）大型语言模型（例如 GPT-3）都是基于因果建模目标（causal modeling objective）进行预训练的，本质上是作为下一个词的预测器。这些 LLM 接收一系列的 token 作为输入，并以自回归（autoregressive）的方式生成后续的 token，直到满足某种停止条件（如生成 token 的数量上限或包含在停止词列表中），或者直到生成一个特殊的 <end> token，表示生成结束。

这个过程包含两个阶段：预填充阶段（prefill phase）和解码阶段（decode phase）。

需要注意的是，**token（标记）**是模型处理的语言最小单元。一个 token 大约相当于四个英文字符。自然语言中的所有输入内容在送入模型之前都会被转换成 token。

### 预填充阶段（Prefill phase）或输入处理阶段

在预填充阶段，大型语言模型（LLM）会处理输入的 token，用以计算中间状态（即键 keys 和值 values），这些状态将用于生成第一个新 token。每一个新生成的 token 都依赖于此前所有的 token，但由于在这个阶段输入是已知的，模型可以对整个输入进行并行处理。

从高层次来看，这一过程相当于是一次矩阵与矩阵之间的运算（matrix-matrix operation），可以高度并行化，因此能够充分利用 GPU 的计算资源，实现计算资源的饱和（即高效利用）。

### 解码阶段（Decode phase）或生成输出

在解码阶段，LLM 以自回归方式一次生成一个输出 token，直到满足某个停止条件为止。每一个新生成的 token 都需要依赖于前面所有已生成的中间状态（即 keys 和 values）。这类似于矩阵-向量运算（matrix-vector operation），相比预填充阶段的矩阵-矩阵运算，这种计算方式对 GPU 的利用率要低很多。

在这个阶段，延迟的主要瓶颈不在于计算速度，而在于数据（如权重、keys、values、激活值）从内存传输到 GPU 的速度。换句话说，解码阶段是一个**受内存带宽限制（memory-bound）**的操作。

本篇文章中提出的许多推理挑战和对应的解决方案，核心都是为了优化这个解码阶段，例如：高效的注意力机制模块；更有效的 keys 和 values 管理策略；以及其他优化手段。

此外，不同的 LLM 可能使用不同的 tokenizer（分词器），因此直接比较它们的输出 token 数并不总是公平的。即使两个模型在“每秒生成 token 数”上看起来相似，它们的推理吞吐量仍可能不同，因为每个 token 所代表的字符数量可能不一样。换句话说，token 数不等于字符数，因此实际处理的语言信息量可能不同。

## 批处理（Batching）

提高 GPU 利用率和推理吞吐量最简单、最直接的方法之一就是使用批处理。由于多个请求共享同一个模型，模型权重的内存开销可以在这些请求之间分摊。将更大的批次一次性传输到 GPU 并同时处理，能够更充分地利用计算资源。

然而，批处理大小（batch size）并不是无限制地增加的。超过某个阈值后，就可能导致内存溢出（memory overflow）。要理解这是为什么，我们需要深入了解 键值缓存（key-value caching） 和大型语言模型的内存需求。

静态批处理的局限（Static Batching）
传统的批处理方式通常是静态批处理，它存在一些明显的不足：批中的每个请求可能生成不同数量的 token（即补全长度不一）；所以它们的推理时间不同；这意味着：整个批次都必须等待最慢的那个请求完成；如果生成长度差异较大，这种等待效应就更严重，从而拉低整体吞吐量。

动态批处理解决方案（预告）
为了解决静态批处理的效率低下问题，有一种更先进的方法叫做动态批处理（in-flight batching），它能够根据请求状态实时调整批次，减少空闲等待时间。这种方法将在后文详细讨论。
简而言之：批处理能显著提升推理效率，但静态批处理效率不高，容易受到生成长度不均衡的影响。理解 KV 缓存和内存开销是优化批处理策略的关键。

## K-V 缓存（Key-value caching）

解码阶段的一种常见优化方法是 KV 缓存（Key-Value Caching）。在解码阶段，每个时间步只生成一个 token，但每个 token 都依赖于之前所有 token 的 key 和 value 张量（包括预填充阶段计算出的输入 token 的 KV 张量，以及直到当前时间步为止新计算出的 KV 张量）。

为了避免在每个时间步都重新计算所有这些张量，可以将它们缓存到 GPU 内存中。每次迭代时，新计算出的元素会被添加到已有的缓存中，以供下一次迭代使用。在某些实现中，模型的每一层都有一个对应的 KV 缓存。

![](/commons/LLM/翻译/1.png)

## LLM 的内存需求

实际上，大型语言模型（LLM）在 GPU 上的主要内存开销来源于两个部分：模型权重（Model weights） 和 键值缓存（KV cache）。

#### 模型权重：

模型参数本身会占用显存。

例如，一个拥有 70 亿参数的模型（如 Llama 2 7B），如果使用 16 位精度（FP16 或 BF16）加载，其内存占用大约为：
7B × sizeof(FP16) ≈ 14 GB

#### KV 缓存（Key-Value Caching）：

用于缓存自注意力机制中的张量，以避免重复计算；在进行批处理时，每个请求的 KV 缓存必须单独分配，因此整体内存开销很大。

📌 KV 缓存大小计算公式：

```
单个 token 的 KV 缓存大小（字节）= 2 × (层数 num_layers) × (注意力头数 × 每个头的维度) × 每个元素的字节数

* 其中「2」代表需要存储 Key 和 Value 两个矩阵；
* 通常 (num_heads × dim_head) 等于 Transformer 的 hidden_size（即模型的总维度 d_model）；
* 这些参数可以在模型卡或配置文件中找到。

```

📌 批次整体 KV 缓存总大小计算公式（假设使用半精度）：

```
总大小（字节）= batch_size × sequence_length × 2 × num_layers × hidden_size × sizeof(FP16)
```

```
示例（Llama 2 7B，16 位精度，batch size = 1）：
假设：序列长度为 4096，32 层 Transformer，hidden size 为 4096。

计算如下：
KV 缓存大小 ≈ 1 × 4096 × 2 × 32 × 4096 × 2 bytes ≈ 2 GB
```

⚠️ 挑战与动机：
KV 缓存的内存需求会随批大小和序列长度线性增长；所以它很容易变得庞大，限制了可处理的吞吐量；同时也给支持长上下文输入带来了挑战；
正因如此，本文后续将讨论多种优化方法，旨在更高效地管理和压缩 KV 缓存。

## 通过模型并行化扩展 LLM 的规模

降低每个设备上模型权重所占内存的一种方法是：将模型分布到多个 GPU 上运行。通过分摊内存和计算负载，可以运行更大的模型，或者处理更大的输入批次。

当模型所需内存超出单个设备的容量时，模型并行化是训练或推理的必需手段。它还可以优化训练时间和推理性能（如延迟或吞吐量），以满足特定的应用需求。根据模型权重的划分方式，有多种模型并行化的方法。

需要注意的是，**数据并行（Data Parallelism）**也是一种常被提及的并行技术，通常与以下方法一同使用。在数据并行中，模型的权重会被复制到多个设备上，而输入的整体批次（global batch size）会在设备之间被划分为多个小批次（microbatches）进行处理。

数据并行能够通过处理更大的批次来减少总体执行时间，但它主要用于训练阶段的优化，在推理阶段的作用相对较小。

### 流水线并行（Pipeline Parallelism）

流水线并行是将模型按“纵向”切分为多个块（chunk），每个块包含一部分层，并在不同的设备上分别执行。图 2a 展示了一个四路流水线并行的示意图：模型被顺序划分为四个部分，每台设备执行全部层的四分之一。某一设备上执行的一组操作的输出会传递给下一台设备，后者继续执行接下来的模型部分。图中的 Fₙ 和 Bₙ 分别表示第 n 台设备上的前向传播（Forward）和反向传播（Backward）。
这种方法的优势在于：每台设备只需要存储一部分模型的权重，因此每个设备的内存需求大约减少为原来的四分之一。

主要限制：
这种方式存在的主要问题是其处理过程是顺序执行的，这就导致：在等待上一部分输出（激活值或梯度）时，后续的某些设备或层会处于空闲状态，从而造成效率低下的问题，这种情况被称为“流水线气泡（pipeline bubbles）”。
图 2b 中的白色区域就是在使用基础流水线并行时出现的流水线气泡，此时设备处于空闲，资源未被有效利用。

缓解方式：微批处理（Microbatching）
图 2c 展示了如何通过**微批处理（microbatching）**来缓解流水线气泡的问题：将全局批次（global batch size）拆分成多个微批次；
每个微批次依次进入流水线处理，最终进行梯度累积；图中的 Fₙ,ₘ 和 Bₙ,ₘ 分别表示第 n 台设备上处理第 m 个微批次的前向和反向传播。这种方法能够减少流水线气泡的空闲时间，提高设备利用率，但并不能完全消除这些空闲现象。

![](/commons/LLM/翻译/2.png)

### 张量并行（Tensor parallelism）
张量并行是指将模型的各个层在水平方向上进行切分，划分为更小的、彼此独立的计算块，并分配到不同的设备上并行执行。注意力模块（attention blocks）和多层感知机（MLP）是变换器（transformer）中的两个主要组成部分，它们都可以很好地利用张量并行技术。
在多头注意力模块中，可以将每个注意力头或一组注意力头分配到不同的设备上，这样它们就可以相互独立地并行计算。

图 3a 展示了一个在两层多层感知机（MLP）上应用的二维张量并行的示例，每一层用一个圆角框表示。在第一层中，权重矩阵 A 被划分为 A₁ 和 A₂。输入 X 与 A₁ 和 A₂ 的乘法计算（XA₁ 和 XA₂）可以在两个不同的设备上并行独立执行，且作用于相同的输入批次（其中函数 f 是恒等操作）。这种方式有效地将每个设备上存储权重所需的内存减半。随后，通过一个归约操作 g，在第二层中将这些计算结果合并。

图 3b 展示了一个在自注意力层中应用二维张量并行的示例。由于多个注意力头天生具有并行性，因此可以将它们划分并分配到多个设备上执行。

![](/commons/LLM/翻译/3.png)

### 