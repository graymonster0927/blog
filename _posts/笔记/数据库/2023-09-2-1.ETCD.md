---
title: etcd
date: 2023-09-02
categories: [笔记, 数据库, DB]
tags: [数据库]
---


# ETCD(V3)

## 概述
> Etcd是CoreOS基于Raft开发的分布式key-value存储，可用于服务发现、共享配置以及一致性 保障(如数据库选主、分布式锁等)。
在分布式系统中，如何管理节点间的状态一直是一个难题，etcd像是专门为集群环境的服务发现 和注册而设计，它提供了数据TTL失效、数据改变监视、多值、目录监听、分布式锁原子操作等 功能，可以方便的跟踪并管理集群节点的状态。

* 键值对存储:将数据存储在分层组织的目录中，如同在标准文件系统中 
* 监测变更:监测特定的键或目录以进行更改，并对值的更改做出反应
* 简单: curl可访问的用户的API(HTTP+JSON)
* 安全: 可选的SSL客户端证书认证
* 快速: 单实例每秒1000次写操作，2000+次读操作 
* 可靠: 使用Raft算法保证一致性

## 主要功能

* 基本的key-value存储
* 监听机制
* key的过期及续约机制，用于监控和服务发现
* 原子Compare And Swap和Compare And Delete，用于分布式锁和leader选举

## 使用场景

* 也可以用于键值对存储，应用程序可以读取和写入 etcd 中的数据 
* etcd 比较多的应用场景是用于服务注册与发现
* 基于监听机制的分布式异步系统

## 数据存储

> * etcd被设计为能够可靠存储不经常更新的数据, 同时提供可靠的watch功能
> * etcd对外能够访问键值对的早期版本，以很小的开销支持的快照和监视历史事件（“时间旅行查询”）。
> * etcd的数据存储结构非常适合持久的、多版本的并发控制数据。

![](/commons/数据库/image/1(1).png)

Etcd v3 将 watch 和 store 拆开实现

### store
 Etcd v3 store 分为两部分, 
 一部分是内存中的索引，kvindex，是基于 google 开源的一 个 golang 的 btree 实现的。
 另外一部分是后端存储。按照它的设计，backend 可以对接 多种存储，当前使用的 boltdb。

#### 1.kvindex (基于 B 树的二级索引)
 > 了解了 Etcd 的磁盘存储，可以看出如果要从 boltdb 中查询数据，必须通过 revision， 但客户端都是通过 key 来查询 value，所以 Etcd 的内存 kvindex 保存的就是 key 和 revision 之前的映射关系，用来加速查询。
 
该 B 树 关键字（key） 是 etcd v3 存储的 数据的key，value是指向 B＋树版本号指针 。如果一个数据key被多次操作（有多个版本操作记录），所有的key都会保存在这个B树节点取值中。大致结构如下：
![](/commons/数据库/image/1(3).png)
 
#### 2.boltdb (B+树)
 > boltdb 是一个单机的支持事务的 kv 存储，Etcd 的事务 是基于 boltdb 的事务实现的。Etcd 在 boltdb 中存储的 key 是 revision，value 是 Etcd 自己的 key-value 组合，也就是说 Etcd 会在 boltdb 中把每个版本都保存下，从而实现 了多版本机制

```
示例：
分别通过两个事务，写入 key1 和 key2 的值，则 boltDB中保存了四条记录如下
put keyl ”vl" put key2”v2”
put keyl ”vl1" put key2”v22”，

BoltDB树形结构中节点信息如下：
rev={3 0),key=keyl,value=” v l ”
rev={3 1) ,key=key2,value=”v2”
rev={4 0),key=keyl,value=” v l 2 "
rev={4 1),key=key2,value=”v22"

revision 主要由两部分组成，第一部分 main rev，每次事务进行加一，第二部分 sub rev，同一个事务中的每次操作加一。如上示例，第一次操作的 main rev 是 3，第二次 是 4。当然这种机制大家想到的第一个问题就是空间问题，所以 Etcd 提供了命令和设置 选项来控制 compact，同时支持 put 操作的参数来精确控制某个 key 的历史版本数。
B＋树中叶子节点的 value 包含了本次修改的某个键值的内容,包括：key、value、key创建的revision等。
B＋树按 版本号字节序进行排序 。 etcd d 对 revision 增量的范围 查询会很快
```
![](/commons/数据库/image/1(2).png)

##### etcd为什么选择boltdb

底层的存储引擎一般包含如下三大类的选择 ：
* SQL Lite 等 SQL 数据库 。
* LevelDB 和 RocksDB 。
* LMDB 和 BoltDB 。

> SQL Lite 支持 ACID 事务。但是作为一个关系型数据库，SQL Lite主 要定位于提供高效灵活的 SQL 查询语句支持，可以支持复杂的联表查询等。而 etcd 只是一个简单的KV 数据库，并不需要复杂的SQL 支持。

> LevelDB 和 RocksDB 分别是 Google 和 Facebook 开发的存储引 擎， RocksDB 是在 LevelDB 的 基础上针对 Flash 设备做了优化。基本原理是将有序的 key/value 存 储在不同的文件中，并通过“层级”将它们分开，并且周期性地将小的文件合 并为大的文件，这样做就能把随机写转化为顺序写，从而提高随机写的性能，因此特别适合“写多读少”和“随机写多”的场景。LevelDB 和 RocksDB 都不支持完整的ACID 事务。

> LMDB 和 BoltDB 则是基于 B 树和 mmap的数据库，基本原理是用 mmap 将磁盘的 page 映射到内存的 page ，而操作系统是通过 COW (copy-on-write) 技术进行 page管理，通过 cow 技术，系统可实现无锁的读写并发，但是无法 实现无锁的写写并发，这类数据库读性能超高，但写性能一般，因 此非常适合于 “读多写少”的场景。同时BoltDB 支持完全可序列化的ACID 事务。因此最适合作为etcd 的底层存储引擎。

### watch(IntervalTree)
然后我们再分析下 watch 机制的实现。Etcd v3 的 watch 机制支持 watch 某个固定的 key，也支持 watch 一个范围(可以用于模拟目录的结构的 watch)，所以 watchGroup 包含两种 watcher，一种是 key watchers，数据结构是每个 key 对应一组 watcher，另 外一种是 range watchers, 数据结构是一个 IntervalTree(不熟悉的参看文文末链接)， 方便通过区间查找到对应的 watcher

同时，每个 WatchableStore 包含两种 watcherGroup，一种是 synced，一种是 unsynced，前者表示该 group 的 watcher 数据都已经同步完毕，在等待新的变更，后者 表示该 group 的 watcher 数据同步落后于当前最新变更，还在追赶。
当 Etcd 收到客户端的 watch 请求，如果请求携带了 revision 参数，则比较请求的 revision 和 store 当前的 revision，如果大于当前 revision，则放入 synced 组中，否则 放入 unsynced 组。同时 Etcd 会启动一个后台的 goroutine 持续同步 unsynced 的watcher，然后将其迁移到 synced 组（备注：客户端请求的revision大于当前的revision说明是监控接下来的更新信息，而不是之前的信息，因此已经完成同步）。也就是这种机制下，Etcd v3 支持从任意版本开 始 watch，没有 v2 的 1000 条历史 event 表限制的问题(当然这是指没有 compact 的 情况下)
另外我们前面提到的，Etcd v2 在通知客户端时，如果网络不好或者客户端读取比较 慢，发生了阻塞，则会直接关闭当前连接，客户端需要重新发起请求。Etcd v3 为了解 决这个问题，专门维护了一个推送时阻塞的 watcher 队列，在另外的 goroutine 里进行 重试。
Etcd v3 对过期机制也做了改进，过期时间设置在 lease 上，然后 key 和 lease 关联。 这样可以实现多个 key 关联同一个 lease id，方便设置统一的过期时间，以及实现批量 续约


## etcd中的raft协议
![](/commons/数据库/image/1(6).png)

### [raft](其他/raft.md)


## etcd Learner
Learner角色只接收数据而不参与投票，因此增加 learner节点时，集群的quorum不变。

详见 -> https://etcd.io/docs/v3.6/learning/design-learner/

## etcd Client
* clientv3-grpc1.23:
  * 和所有node维持心跳检测连接
  * 多种失败重试策略
  * client内自动对失败重试
  * 但仍然没有解决对node分区异常的主动检测
    
## 高可用/故障恢复


## 对比其他kv存储
![](/commons/数据库/image/1(4).png)
![](/commons/数据库/image/1(5).png)


### ZooKeeper
ZooKeeper solves the same problem as etcd: distributed system coordination and metadata storage. However, etcd has the luxury of hindsight taken from engineering and operational experience with ZooKeeper’s design and implementation. The lessons learned from Zookeeper certainly informed etcd’s design, helping it support large scale systems like Kubernetes. The improvements etcd made over Zookeeper include:

* Dynamic cluster membership reconfiguration
* Stable read/write under high load
* A multi-version concurrency control data model
* Reliable key monitoring which never silently drop events
* Lease primitives decoupling connections from sessions
* APIs for safe distributed shared locks

Furthermore, etcd supports a wide range of languages and frameworks out of the box. Whereas Zookeeper has its own custom Jute RPC protocol, which is totally unique to Zookeeper and limits its supported language bindings, etcd’s client protocol is built from gRPC, a popular RPC framework with language bindings for go, C++, Java, and more. Likewise, gRPC can be serialized into JSON over HTTP, so even general command line utilities like curl can talk to it. Since systems can select from a variety of choices, they are built on etcd with native tooling rather than around etcd with a single fixed set of technologies.

When considering features, support, and stability, new applications planning to use Zookeeper for a consistent key value store would do well to choose etcd instead.

(核心算法是 ZAB 是 CP , 作为服务发现会有问题, 且是自己的 rpc 协议 , etcd 是 grpc )
### Consul
Consul is an end-to-end service discovery framework. It provides built-in health checking, failure detection, and DNS services. In addition, Consul exposes a key value store with RESTful HTTP APIs. As it stands in Consul 1.0, the storage system does not scale as well as other systems like etcd or Zookeeper in key-value operations; systems requiring millions of keys will suffer from high latencies and memory pressure. The key value API is missing, most notably, multi-version keys, conditional transactions, and reliable streaming watches.

etcd and Consul solve different problems. If looking for a distributed consistent key value store, etcd is a better choice over Consul. If looking for end-to-end cluster service discovery, etcd will not have enough features; choose Kubernetes, Consul, or SmartStack.

### Nacos
(多数据中心部署)

## SeeMore
* [Etcd 架构与实现解析](https://jolestar.com/etcd-architecture/)
* [etcd versus other key-value stores](https://etcd.io/docs/v3.5/learning/why/)
* [全方位对比Zookeeper、Eureka、Nacos、Consul和Etcd](https://blog.csdn.net/qq_42046105/article/details/123436832)
* [斗鱼直播云原生实践之注册中心篇](https://developer.baidu.com/article/detail.html?id=293981)

 
