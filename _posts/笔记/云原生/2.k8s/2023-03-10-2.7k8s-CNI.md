---
title: 2.7 CNI
date: 2023-03-10
categories: [笔记, '云原生', 'k8s']
tags: [云原生]
---

# CNI

## 什么是CNI
Kubernetes网络模型设计的基础原则是：
* 所有的Pod能够不通过NAT就能相互访问。
* 所有的节点能够不通过NAT就能相互访问。
* 容器内看见的IP地址和外部组件看到的容器IP是一样的。

Kubernetes的集群里，IP地址是以Pod为单位进行分配的，每个Pod都拥有一个独立的IP地址。<br>
一个Pod内部的所有容器共享一个网络栈，即宿主机上的一个网络命名空间，包括它们的IP地址、网络 设备、配置等都是共享的。也就是说，Pod里面的所有容器能通过localhost:port来连接对方。<br>
在Kubernetes中,提供了一个轻量的通用容器网络接口 CNI (Container Network Interface),专门 用于设置和删除容器的网络连通性。容器运行时通过CNI调用网络插件来完成容器的网络设置。

## CNI插件分类和常见插件
* IPAM： IP地址分配
* 主插件：网卡设置
  * bridge:创建一个网桥，并把主机端口和容器端口插入网桥
  * ipvlan:为容器添加ipvlan网口
  * Loopback:设置loopback网口
* Meta:附加功能
  * portmap:设置主机端口和容器端口映射
  * bandwidth:利用Linux Traffic Control限流
  * firewall:通过iptables或firewalld为容器设置防火墙规则

https://github.com/containernetworking/plugins

## CNI运行机制

容器运行时在启动时会从CNI的配置目录中读取JSON格式的配置文件，文件后缀为 .conflist/.json...<br>
如果配置目录中包含多个文件，一般情况下，会以名字排序选用第 一个配置文件作为默认的网络配置，并加载获取其中指定的CNI插件名称和配置参数。

![](/commons/云原生/docker/image/2.7(1).png)

关于容器网络管理，容器运行时一般需要配置两个参数-cni-bin-dir和-cni-conf-dir<br>
有一种特殊情况，kubelet内置的Docker作为容器运行时，是由kubelet来查找CNI插件的，运行插件来为容器设置网络，这两个参数应该配置在kubelet处：
cni-bin-dir:网络插件的可执行文件所在目录。默认是/opt/cni/bin<br>
cni-conf-dir:网络插件的配置文件所在目录。默认是/etc/cni/net.d<br>


## CNI插件设计考量

* 容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。
* 容器运行时必须决定这个容器属于哪些网络,针对每个网络,哪些插件必须要执行。
* 容器运行时必须加载配置文件，并确定设置网络时哪些插件必须被执行。
* 网络配置采用JSON格式，可以很容易地存储在文件中。
* 容器运行时必须按顺序执行配置文件里相应的插件。
* 在完成容器生命周期后,容器运行时必须按照与执行添加容器相反的顺序执行插件,以便将容器与网络断开连接。
* 容器运行时被同一容器调用时不能并行操作，但被不同的容器调用时,允许并行操作。
* 容器运行时针对一个容器必须按顺序执行ADD和DEL操作,ADD后面总是跟着相应的DEL, DEL可能跟着额外的DEL,插件应该允许处理多个DEL
* 容器必须由ContainerlD来唯一标识,需要存储状态的插件需要使用网络名称、容器ID和网络接口组成的主key用于索引。
* 容器运行时针对同一个网络、同一个容器、同一个网络接口,不能连续调用两次 ADD命令。

## sandbox
容器运行时必须在调用任何插件之前为容器创建一个新的网络命名空间。
k8s 在启动一个 pod 的时候实际上会启动两个容器，一个 sandbox containner 和 user containner<br>
事实上，k8s会先启动这么个sandbox，但只执行一个操作 sleep ，即容器直接进入睡眠状态，意味着这个镜像几乎不占用任何资源，切极度稳定不会崩溃。<br>
创建独立的 network namespace，并将其与sandbox的进程相关联，其作用是：<br>
2.1. 在启动user containner 时我们可能需要下载一些依赖，所以网络必须提前配置好<br>
2.2. user containner 中进程可能会应为程序的异常而崩溃，容器重启时网络配置就需要重新设置。而k8s 用这个永远不会崩溃的容器 sandbox 来规避这种现象。<br>


## 打通主机层网络
CNI插件外, Kubernetes还需要标准的CNI插件lo,最低版本为0.2.0版本。网络插件除支持设置和清理Pod网络接口外，该插件还需要支持lptables<br>
如果Kube-proxy工作在Iptables模式，网络插 件需要确保容器流量能使用Iptables转发。<br>
例如/如果网络插件将容器连接到Linux网桥，必须将 net/bridge/bridge-nf-call-iptables参数sysctl设置为1,网桥上数据包将遍历Iptables规则。<br>
如果插件不使用Linux桥接器（而是类似。pen vSwitch或其他某种机制的插件）,则应确保容器流量被正确设置了路由。


> CNI Plugin ContainerNetworking 组维护了一些 CNI 插件， 包括网络接口创建的 bridge、ipvlan、loopback、 macvlan. ptp、host-device 等, IP 地址分配的 DHCP、host-local 和 static,其他的 Flannek tunning, portmap. firewall等。
> 社区还有些第三方网络策略方面的插件,例如Calico. Cilium和Weave等。可用选项的多样性意味 着大多数用户将能够找到适合其当前需求和部署环境的CNI插件，并在情况变化时迅捷转换解决方案。

## 一些plugin

### Flannel
Flannel是由CoreOS开发的项目,是CNI插件早期的入门产品/简单易用。<br>
Flannel使用Kubernetes集群的现有etcd集群来存储其状态信息, 从而不必提供专用的数据存储 只需要在每个节点上运行flanneld来守护进程。<br>
每个节点都被分配一个子网，为该节点上的Pod分配IP地址。<br>
同一主机内的Pod可以使用网桥进行通信/而不同主机上的Pod将通过flanneld将其流量封装在UDP数据包中,以路由到适当的 目的地。
封装方式默认和推荐的方法是使用VxLAN,因为它具有良好的性能/并且比其他选项要少些人为干预。虽然使用VxLAN之类的技术 封装的解决方案效果很好,但缺点就是该过程使流量跟踪变得困难。

![](/commons/云原生/docker/image/2.7(2).png)

### Calico
Calico以其性能、灵活性和网络策略而闻名，不仅涉及在主机和Pod之间提供网络连接，而且还涉及网络安全性和策略管理。<br>
对于同网段通信，基于第3层，Calico使用BGP路由协议在主机之间路由数据包，使用BGP路由协议也意味着数据包在主机之间移动时不需要包装在额外的封装层中。<br>
对于跨网段通信，基于IPinlP使用虚拟网卡设备tunlO, 用一个IP数据包封装另一个IP数据包，外层IP数据包头的源地址为隧道入口设备的IP地址，目标地址为隧道出口设备的IP地址。<br>
网络策略是Calico最受欢迎的功能之一,使用ACLs协议和kube-proxy来创建iptables过滤规则，从而实现隔离 容器网络的目的。<br>
此外，Calico还可以与服务网格Istio集成，在服务网格层和网络基础结构层上解释和实施集群中工作负载的策略。 这意味着您可以配置功能强大的规则，以描述Pod应该如何发送和接收流量，提高安全性及加强对网络环境的控制。<br>
Calico属于完全分布式的横向扩展结构，允许开发人员和管理员快速和平稳地扩展部署规模。对于性能和功能（如网络策略）要求高的环境，Calico是一个不错选择。<br>
![](/commons/云原生/docker/image/2.7(3).png)

## CNI plugins 对比
![](/commons/云原生/docker/image/2.7(4).png)

## SeeMore
[K8S CNI(wolai)](https://www.wolai.com/okhkx1hEaQmjUKerTk42HF)
