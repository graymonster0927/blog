 ---
title: transformer
date: 2025-07-28
categories: [ç¬”è®°, LLM]
tags: [LLM]
---


# **Transformer ä» Encoder åˆ° Decoder çš„å®Œæ•´è¿‡ç¨‹**

> **é‡ç‚¹å‰æï¼š**
> ä½ è¾“å…¥çš„æ˜¯ä¸€ä¸ªå¥å­ï¼ˆ10ä¸ª tokenï¼Œæ¯ä¸ªæ˜¯ 512 ç»´å‘é‡ï¼‰ï¼Œé‡‡ç”¨ 8 ä¸ªæ³¨æ„åŠ›å¤´ã€‚

---

## ğŸŒ æ•´ä½“ç»“æ„ï¼ˆEncoder â†’ Decoderï¼‰

```text
          [Input Tokens]                [Target Tokens]
               â”‚                             â”‚
          Embedding                     Embedding (shifted)
               â”‚                             â”‚
          Positional Encoding          Positional Encoding
               â”‚                             â”‚
           Encoder Blocks â”€â”€â”€â”€â”€â”     Decoder Blocks
               â†“              â”‚         â”‚
     Encoder Output (memory)  â””â”€â”€â–º Encoder-Decoder Attention
                                         â†“
                                Linear + Softmax â†’ ä¸‹ä¸€ä¸ª token
```

---

## ğŸ”§ æˆ‘ä»¬å‡è®¾è¾“å…¥ï¼š

* æºå¥å­ï¼ˆè‹±æ–‡ï¼‰é•¿åº¦ = 10ï¼Œç»´åº¦ = 512 â†’ `X_src âˆˆ [10, 512]`
* ç›®æ ‡å¥å­ï¼ˆæ³•è¯­ï¼‰é•¿åº¦ = 8ï¼ˆé€æ­¥ç”Ÿæˆï¼‰ â†’ `X_tgt âˆˆ [8, 512]`
* å¤šå¤´æ•° = 8ï¼Œæ¯å¤´ç»´åº¦ = 64

---

## ğŸ§± ç¬¬ä¸€æ­¥ï¼šEncoder

### è¾“å…¥ï¼š

```text
X_src: [10, 512]
```

### ç¼–ç å™¨ç»“æ„ï¼ˆæ¯å±‚ï¼‰ï¼š

#### â‘  Self-Attentionï¼ˆQ=K=V=X\_srcï¼‰

* æŠ•å½±ï¼š

  $$
  Q = X W^Q,\quad K = X W^K,\quad V = X W^V \quad \text{â†’ shape: } [10, 512]
  $$
* æ‹†æˆ 8 ä¸ªå¤´ï¼š

  $$
  Q_h, K_h, V_h âˆˆ [8, 10, 64]
  $$
* æ³¨æ„åŠ›è®¡ç®—ï¼š

  $$
  \text{Attention}_h = \text{softmax}(Q_h K_h^T / \sqrt{64}) V_h \quad â†’ [8, 10, 64]
  $$
* åˆå¹¶ï¼š

  $$
  \text{Concat}_{h=1}^8 â†’ [10, 512]
  $$
* è¾“å‡ºçº¿æ€§å˜æ¢ï¼š

  $$
  O = \text{Concat}(...) W^O âˆˆ [10, 512]
  $$

#### â‘¡ Add + Norm + FeedForward + Norm

è¾“å‡ºä¿æŒåœ¨ `[10, 512]`ï¼Œé€å…¥ä¸‹ä¸€å±‚ï¼ˆé€šå¸¸å †å  6 å±‚ï¼‰

æœ€ç»ˆï¼š

```text
Encoder output E âˆˆ [10, 512] ï¼ˆè¯­ä¹‰è¡¨ç¤ºï¼‰
```

---

## ğŸ§± ç¬¬äºŒæ­¥ï¼šDecoder

### è¾“å…¥ï¼š

```text
X_tgt: [1, 512] ï¼ˆç›®æ ‡æœ€åä¸€ä¸ªtoken, å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡ç”¨<BOS>/<s>ï¼‰
E: [10, 512] ï¼ˆencoder çš„è¾“å‡ºï¼‰
```

### è§£ç å™¨ç»“æ„ï¼ˆæ¯å±‚ï¼‰ï¼š

#### â‘  Masked Self-Attentionï¼ˆQ=K=V=X\_tgtï¼‰

* å’Œ Encoder çš„ attention ä¸€æ ·ï¼Œä½†åŠ äº† causal mask åªçœ‹è‡ªå·±å’Œå‰é¢ï¼š
* è¿™é‡Œæ¯”å¦‚ç”Ÿæˆäº†8ä¸ªtoken, å‰7ä¸ªå·²ç»ç¼“å­˜äº†K V

```text
è¾“å…¥ï¼š
X_tgt: [1, 512]
â†“ Linear â†’ Q, K, V âˆˆ [1, 512]
â†“ reshape â†’ 8 ä¸ªå¤´ â†’ [1, 8, 64]
â†“ QK^T + mask â†’ softmax â†’ attention â†’ ä¹˜ V
â†“ æ‹¼æ¥ â†’ [1, 512]
```

---

#### â‘¡ ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›ï¼ˆQ=X\_tgt, K=V=Eï¼‰

è¿™æ˜¯é‡ç‚¹ï¼

* **Q** æ¥è‡ª decoder å½“å‰ç”Ÿæˆçš„ token å‘é‡ `X_tgt`
* **K, V** æ¥è‡ª encoder è¾“å‡ºçš„ä¸Šä¸‹æ–‡è¡¨ç¤º `E`

```text
Q = X_tgt W^Q â†’ [1, 512]
K = E W^K     â†’ [10, 512]
V = E W^V     â†’ [10, 512]
â†“ reshape æˆå¤šå¤´
Q âˆˆ [1, 8, 64], K/V âˆˆ [1, 10, 64]
â†“ è®¡ç®— QK^T â†’ softmax â†’ ä¹˜ä¸Š V
â†“ è¾“å‡º [1, 512]
```

è¿™ä¸€æ­¥æ˜¯ Decoder â€œè¯»å–â€ Encoder ç¼–ç å¥½çš„ä¸Šä¸‹æ–‡ã€‚

---

#### â‘¢ FFN + Add + Norm

ç»´åº¦ä»ç„¶æ˜¯ `[1, 512]`

---

### æœ€ç»ˆï¼š

é€å…¥ Linear + Softmaxï¼š

```text
output = Linear([1, 512]) â†’ softmax([1, vocab_size]) â†’ ä¸‹ä¸€ä¸ª token æ¦‚ç‡åˆ†å¸ƒ
```

---



## ğŸ”„ æ¯æ­¥ç”Ÿæˆä¸€ä¸ª tokenï¼š

Decoder æ˜¯è‡ªå›å½’çš„ï¼Œæ¯æ¬¡é¢„æµ‹ä¸€ä¸ª tokenï¼Œå†å–‚å›å»ç»§ç»­é¢„æµ‹ä¸‹ä¸€ä¸ªã€‚

---

## âœ… æ€»ç»“ QKV çš„æ¥æºå¯¹æ¯”

| æ¨¡å—                   | Query æ¥æº      | Key/Value æ¥æº  | æ˜¯å¦ Mask |
| -------------------- | ------------- | ------------- | ------- |
| Encoder Self-Attn    | Encoder è¾“å…¥    | Encoder è¾“å…¥    | âŒ No    |
| Decoder Self-Attn    | Decoder å·²ç”Ÿæˆéƒ¨åˆ† | Decoder å·²ç”Ÿæˆéƒ¨åˆ† | âœ… Yes   |
| Encoder-Decoder Attn | Decoder       | Encoder è¾“å‡º    | âŒ No    |

---

## âœ… è¾“å…¥ç»´åº¦å˜åŒ–æ€»è§ˆ

| é˜¶æ®µ                 | ç»´åº¦                |
| ------------------ | ----------------- |
| è¾“å…¥ token embedding | `[10, 512]`       |
| Encoder è¾“å‡º         | `[10, 512]`       |
| Decoder è¾“å…¥         | `[1, 512]`        |
| Decoder è¾“å‡º         | `[1, 512]`        |
| Linear + Softmax   | `[1, vocab_size]` |

---
