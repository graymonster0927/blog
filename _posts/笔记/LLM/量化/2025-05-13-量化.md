---
title: 量化
date: 2025-05-13
categories: [笔记, LLM, 量化]
tags: [LLM]
---

> 本文内容抄自 [@https://zhuanlan.zhihu.com/p/693207469](from:沉思的斯多葛九狗)

# 量化类型
## 线性量化和非线性量化
（1）线性量化：均匀的量化，原始数据跟量化后的数据存在一个简单的线性变换关系。

（2）非线性量化：对数据分布密集的区域，给与更多的量化映射，就能增加量化后的差异性，提高精度。
> 相关非线性量化方法：分位量化方法（Quantile Quantization）。LLM QLoRA 算法提出的 NF4(4-bit NormalFloat Quantization) 是分位量化的一种实现。

## 对称量化和非对称量化(线性)
> (在线性量化中，按Z是否等于0，分为对称量化和非对称量化)

### 对称量化
   对于权重w而言，由于数据分布一般为对称，所以采用对称量化。   
   image.png

### 非对称量化
   对于激活值a而言，由于数据分布一般为非对称（大于0的数很多），所以采用非对称量化。
   image.png

## 组量化和通道量化
对于权重W来说，要区分整个权重整体量化和针对权重的组或通道量化。
   整体量化：对于整个权重都使用相同的scale。
   组量化：将权重按组划分，每组使用自己的scale，这样每组内部的精度都能得到保障。
   通道量化：将权重的每个输出通道单独量化，即每个输出通道有自己的scale。

## 动态量化和静态量化
静态量化：在部署在线推理前已经确定好量化参数，如何确定量化参数：
- 数据自由(data-free)：可直接获取（比如weight）或不需要获取其统计特性的张量；
- 校准(calibration)：需要依赖校准数据获取特定的统计特性，比如自定义的函数(比如 min-max, KL-散度等)。

动态量化：推理中运行时才确定量化参数。

# 量化算法
## 权重量化算法
### 基于离群点的量化策略
量化算法之父：BnB（bits and bytes）int8 算法。

核心思想：对超出threshold的离群点，不采用int8数据类型。缺点：离群点难以硬件优化。

### 基于泰勒展开的量化策略
代表：GPTQ

原理：压缩后可能存在loss变大的情况，如何找到替代参数影响最小。泰勒展开：

$$\delta E = \sum_i g_i \delta u_i + \frac{1}{2} \sum_i h_{ii} \delta u_i^2 + \frac{1}{2} \sum_{i \ne j} h_{ij} \delta u_i \delta u_j + O(||{\delta} \mathbf{u}||^3)$$

参数剪枝是参数量化的一种特殊情况（把参数直接置为0这个特殊量化值）。

- OBD考虑了对角近似
- OBS使用了全部Hessian矩阵
- OBC考虑了Hessian矩阵的计算，单独处理每行的剪枝
- OBQ（optimization-based quantization）是将这个思路扩展到量化方法，GPTQ的基础是OBQ

### 基于权重显著性的量化策略
代表：AWQ

核心思想：不是所有权重都同等重要。AWQ提出量化前对权重与激活值进行变换，使得量化误差整体最小。

### 基于权重分布的量化策略
代表：HQQ

方法：设计更好的权重分布函数，使得权重能够被更好地量化（比如让权重值分布更均匀，而不是过于密集或稀疏）。

小结:
1. BnB量化：针对离群值处理，离群值不量化，普通值量化；支持nf4格式（数值呈正态分布）
2. GPTQ量化：结合泰勒展开评估权重重要性，更新量化参数，需要校准数据集
3. AWQ量化：评估激活值重要性后对重要参数进行缩放，需要校准数据集
4. HQQ量化：优化零点量化的算法，不需要校准数据集

## 量化训练方法
QLoRA是在LoRA的基础上进行4bit量化。LoRA是附着在模型上的额外参数矩阵，在训练时冻结原模型，仅训练LoRA部分。

SWIFT框架提供了方便的量化训练选项：
```bash
swift sft --model_type llama3-8b-instruct --dataset alpaca-en --quantization_bit 8 --quant_method bnb --sft_type lora
```

## 常见量化库
### AutoGPTQ
基于GPTQ算法的量化库，需要校准数据，精度高，速度快。不支持合并adapter。

### BitsAndBytes
Data-free量化库，无需校准数据，速度快，支持QLoRA训练和合并adapter，但精度略低于AutoGPTQ。

### GGML/GGUF
适配CPU推理的量化格式，可在CPU上高效运行。

### AutoAWQ
基于AWQ算法的量化库，相比GPTQ精度更高，推理性能更好，VLLM支持较好。


